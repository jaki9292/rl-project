{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSPB 3202 Final Project #\n",
    "\n",
    "Tyler Kinkade, jaki9292@colorado.edu\n",
    "\n",
    "GitHub: [https://github.com/jaki9292/rl-project](https://github.com/jaki9292/rl-project)\n",
    "\n",
    "## Overview ##\n",
    "\n",
    "This write-up reports on a small project to train and test a reinforcement learning algorithm (Russell & Norvig, 2022; Sutton & Barto, 2018) with the Gymnasium (2022) Python software package. \n",
    "\n",
    "This report is divided into the following sections: approach, results, discussion, and suggestions for future research.\n",
    "\n",
    "## Approach ##\n",
    "\n",
    "This section is divided into the following subsections: environment and game rules, models, methods and purpose, and problem solving procedure.\n",
    "\n",
    "### Environment and Game Rules ###\n",
    "\n",
    "Does it explain how the environment works and what the game rules are?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lunar Lander Animated gif](lunar_lander.gif)\n",
    "\n",
    "Source: https://gymnasium.farama.org/_images/lunar_lander.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 total rewards: -109.87890261993823\n",
      "Episode 1 total rewards: -172.7107071245145\n",
      "Episode 2 total rewards: -34.56385957378821\n"
     ]
    }
   ],
   "source": [
    "# Set up and display random agent in lunar lander environment\n",
    "# Adapted from: https://gymnasium.farama.org/\n",
    "# References: \n",
    "# https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "# https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py\n",
    "\n",
    "# Install dependencies\n",
    "# pip install gymnasium\n",
    "# pip install gymnasium[box2d]\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "# Initialize environment\n",
    "env0 = gym.make(\"LunarLander-v2\", \n",
    "               continuous = False,     # Discrete version\n",
    "               gravity = -10.0, \n",
    "               enable_wind = False, \n",
    "               wind_power = 0.0, \n",
    "               turbulence_power = 0.0, \n",
    "               render_mode=\"human\")    # Render for humans\n",
    "\n",
    "# Reset environment with random number generator seed for reproducibility\n",
    "observation, info = env0.reset(seed = 21)\n",
    "\n",
    "# Accumulator variables\n",
    "reward_total = 0.0\n",
    "reward_totals = []\n",
    "episode = 0\n",
    "\n",
    "# Attempt for 300 timesteps\n",
    "for step_index in range(300):\n",
    "\n",
    "    # Get random action from action space\n",
    "    action = env0.action_space.sample()\n",
    "\n",
    "    # Obtain observation, reward, terminated status, truncated status, \n",
    "    # and environment info for given action\n",
    "    observation, reward, terminated, truncated, info = env0.step(action)\n",
    "\n",
    "    # Accumulate reward total \n",
    "    reward_total += reward\n",
    "\n",
    "    # If episode ends\n",
    "    if terminated or truncated:\n",
    "        # Append total to list of reward totals\n",
    "        reward_totals.append(reward_total)\n",
    "\n",
    "        # Report result\n",
    "        print(f\"Episode {episode} total rewards: {reward_total}\")\n",
    "\n",
    "        # Increment epsiode count\n",
    "        episode += 1\n",
    "\n",
    "        # Reset reward total\n",
    "        reward_total = 0\n",
    "        \n",
    "        # Start new episode \n",
    "        observation, info = env0.reset()\n",
    "\n",
    "env0.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 total rewards: -488.55828815343216\n",
      "Moviepy - Building video /home/tyler/Projects/cspb3202/rl-project/videos/random_agent-episode-0.mp4.\n",
      "Moviepy - Writing video /home/tyler/Projects/cspb3202/rl-project/videos/random_agent-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/tyler/Projects/cspb3202/rl-project/videos/random_agent-episode-0.mp4\n",
      "Episode 1 total rewards: -169.94460192169578\n",
      "Moviepy - Building video /home/tyler/Projects/cspb3202/rl-project/videos/random_agent-episode-1.mp4.\n",
      "Moviepy - Writing video /home/tyler/Projects/cspb3202/rl-project/videos/random_agent-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/tyler/Projects/cspb3202/rl-project/videos/random_agent-episode-1.mp4\n",
      "Episode 2 total rewards: -95.39307447237705\n",
      "Episode 3 total rewards: -86.26876262890563\n",
      "Episode 4 total rewards: -143.50653466584134\n",
      "Episode 5 total rewards: -299.1755190702528\n",
      "Episode 6 total rewards: -153.0113786509105\n",
      "Episode 7 total rewards: -232.41193480457278\n",
      "Episode 8 total rewards: -199.0548129015797\n",
      "Moviepy - Building video /home/tyler/Projects/cspb3202/rl-project/videos/random_agent-episode-8.mp4.\n",
      "Moviepy - Writing video /home/tyler/Projects/cspb3202/rl-project/videos/random_agent-episode-8.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/tyler/Projects/cspb3202/rl-project/videos/random_agent-episode-8.mp4\n",
      "Episode 9 total rewards: -435.7639224818247\n"
     ]
    }
   ],
   "source": [
    "# Record video of every k^3th episode (i.e., 0, 1, 8, 27, ...)\n",
    "# Adapted from https://gymnasium.farama.org/api/utils/#save-rendering-videos\n",
    "\n",
    "# Install dependencies\n",
    "# pip install moviepy\n",
    "\n",
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "# Initialize environment\n",
    "env1 = gym.make(\"LunarLander-v2\", \n",
    "               continuous = False,          # Discrete version\n",
    "               gravity = -10.0, \n",
    "               enable_wind = False, \n",
    "               wind_power = 0.0, \n",
    "               turbulence_power = 0.0, \n",
    "               render_mode=\"rgb_array_list\") # Render for machine\n",
    "\n",
    "# Reset environment with random number generator seed for reproducibility\n",
    "observation, info = env1.reset(seed = 21)\n",
    "\n",
    "# Accumulator variables\n",
    "reward_total = 0.0\n",
    "reward_totals = []\n",
    "episode = 0\n",
    "step_start_index = 0\n",
    "\n",
    "# Attempt for 1000 timesteps\n",
    "for step_index in range(1000):\n",
    "\n",
    "    # Get random action from action space\n",
    "    action = env1.action_space.sample()\n",
    "\n",
    "    # Obtain observation, reward, terminated status, truncated status, \n",
    "    # and environment info for given action\n",
    "    observation, reward, terminated, truncated, info = env1.step(action)\n",
    "\n",
    "    # Accumulate reward total \n",
    "    reward_total += reward\n",
    "\n",
    "    # If episode ends\n",
    "    if terminated or truncated:\n",
    "        # Append total to list of reward totals\n",
    "        reward_totals.append(reward_total)\n",
    "\n",
    "        # Report result\n",
    "        print(f\"Episode {episode} total rewards: {reward_total}\")\n",
    "\n",
    "        # Save mp4 video of every (k^3)th (i.e., 0, 1, 8, 27, ...)\n",
    "        save_video(env1.render(),\n",
    "                   video_folder = \"videos\",\n",
    "                   name_prefix = \"random_agent\",\n",
    "                   fps = env1.metadata[\"render_fps\"],\n",
    "                   step_starting_index = step_start_index,\n",
    "                   episode_index = episode)\n",
    "\n",
    "        # Increment episode\n",
    "        episode += 1\n",
    "\n",
    "        # Set starting step index for videos to next step\n",
    "        step_start_index = step_index + 1\n",
    "\n",
    "        # Reset reward total\n",
    "        reward_total = 0\n",
    "        \n",
    "        # Start new episode \n",
    "        observation, info = env1.reset()\n",
    "\n",
    "env1.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models ###\n",
    "\n",
    "Does it explain clearly the model(s) of choices, the methods and purpose of tests and experiments?\n",
    "\n",
    "Approximate reinforcement learning (Russell & Norvig, 2022; Sutton & Barto, 2018)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods and Purpose ###\n",
    "\n",
    "methods and purpose of tests and experiments\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Solving Procedure ###\n",
    "\n",
    "Does it show problem solving procedure- e.g. how the author solved and improved when an algorithm doesn't work well. Note that it's not about debugging or programming/implementation, but about when a correctly implemented algorithm wasn't enough for the problem and the author had to modify/add some features or techniques, or compare with another model, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results ## \n",
    "\n",
    "show the result and interpretation of your experiment. Any iterative improvements summary.\n",
    "\n",
    "demo clips\n",
    "\n",
    "Does it include the results summary, interpretation of experiments and visualization (e.g. performance comparison table, graphs etc)?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion ## \n",
    "\n",
    "Does it include discussion (what went well or not and why), and suggestions for improvements or future work?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggestions for Future Research ##\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References ##\n",
    "\n",
    "Gymnasium. (2022). _Gymnasium documentation._ Farama Foundation. [https://gymnasium.farama.org/](https://gymnasium.farama.org/)\n",
    "\n",
    "Russell, S., & Norvig, R. (2022). Artificial intelligence: A modern approach, (4th ed.). Pearson. \n",
    "\n",
    "Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT Press."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
