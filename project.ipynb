{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSPB 3202 Final Project #\n",
    "\n",
    "Tyler Kinkade, jaki9292@colorado.edu\n",
    "\n",
    "GitHub: [https://github.com/jaki9292/rl-project](https://github.com/jaki9292/rl-project)\n",
    "\n",
    "## Overview ##\n",
    "\n",
    "This write-up reports on a small project to compare the effectiveness of reinforcement learning algorithms (Russell & Norvig, 2022; Sutton & Barto, 2018) within the Gymnasium (2022) \"lunar lander\" environment.\n",
    "\n",
    "The report is divided into the following sections: approach, results, discussion, and suggestions.\n",
    "\n",
    "## Approach ##\n",
    "\n",
    "This section is divided into the following subsections: environment, models, methods.\n",
    "\n",
    "### Environment ###\n",
    "\n",
    "The lunar lander environment (pictured below) simulates rocket trajectory physics with the primary aim of landing a lunar lander on a central landing pad (marked by two flags) by means of turning its three rockets (left, right, and main) on or off. The agent operating the lander is rewarded for landing slowly, in an upright position, on both legs, on or near the landing pad and penalized otherwise. Fuel is unlimited, but a penalty is made for each time an engine fires. Scores of 200 points or more are considered a solution. \n",
    "\n",
    "The observation space is described by an 8-vector comprised of the lander coordinates $(x,y)$, its linear velocities in the $x$ and $y$ directions, its angle, its angular velocity, and a boolean each for whether the left and right leg are in contact with the ground. The environment has both discrete and continuous versions, but only the discrete version is used here to facilitate the comparison of simple models. The environment gravity, wind power, and turbulence can also be specified. The starting state is the top center with a random force to it. The termination state occurs when the lander stops moving or moves outside the frame. The action space is comprised of 4 discrete actions: do nothing, fire left thruster, fire main thruster, or fire right thruster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://gymnasium.farama.org/_images/lunar_lander.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Lunar Lander GIF\n",
    "from IPython.display import Image\n",
    "Image(url= \"https://gymnasium.farama.org/_images/lunar_lander.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment metadata:\n",
      " {'render_modes': ['human', 'rgb_array'], 'render_fps': 50}\n",
      "\n",
      "Observation space:\n",
      " Box([-90.        -90.         -5.         -5.         -3.1415927  -5.\n",
      "  -0.         -0.       ], [90.        90.         5.         5.         3.1415927  5.\n",
      "  1.         1.       ], (8,), float32)\n",
      "\n",
      "Action space:\n",
      " Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "# Print environment parameters\n",
    "# References: \n",
    "# https://gymnasium.farama.org/api/env/\n",
    "# https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "\n",
    "# Install dependencies\n",
    "# pip install gymnasium\n",
    "# pip install gymnasium[box2d]\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make(\"LunarLander-v2\", \n",
    "               continuous = False,      # Discrete version\n",
    "               gravity = -10.0, \n",
    "               enable_wind = False, \n",
    "               wind_power = 0.0, \n",
    "               turbulence_power = 0.0, \n",
    "               render_mode=\"rgb_array\") # Render for machine\n",
    "\n",
    "env.reset()\n",
    "\n",
    "print(\"Environment metadata:\\n\", env.metadata)\n",
    "print(\"\\nObservation space:\\n\", env.observation_space)\n",
    "print(\"\\nAction space:\\n\", env.action_space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models ###\n",
    "\n",
    "Four models are compared in this project: a random agent, a simple reflex agent, a Q-learning agent, and an approximate reinforcement learning agent(Russell & Norvig, 2022; Sutton & Barto, 2018). A random agent selects any of the possible actions uniformly randomly at each step without regard to the environment. The animation above is an example of such an agent, which is obviously a poor strategy. A simple reflex agent can only react to some input but does not retain any memory of the past and, as a result, cannot learn. A Q-learning agent alternates between learning through exploration and exploiting what it has learned by choosing the optimal policy for each state. Although this is obviously better than the relatively ignorant reflex agent, Q-learning agents can quickly use of computational capacity in large and complex environments. An approximate reinforcement learning agent is meant to addresse this issue by approximating a Q-learning agent through the use of a weighted linear approximation of the optimal policy which it learns through exploration a certain amount of the time. It is expected that the last two models will perform better than the first two; however, it is possible that some difficulties might arise with the two learning agents. The environment might be too complex for the Q-learning agent to learn within a reasonable time. It is also possible that approximating the optimal choices may be insufficient. \n",
    "\n",
    "### Methods ###\n",
    "\n",
    "Each model will be run for 10,000 time-steps (approximately 100 episodes) and their scores over each of the episodes will be plotted and compared. Visualizing each models' performance over successive episodes permits comparison of how each agent progressed in learning how to solve the model. This is preferable to single metrics, such as maximum or average scores which can conceal important distinctions. For example, we can discover whether an agent achieved a maximum only once, but performed poorly most of the time. Similarly, an average score can conceal unstable, fluctuating performance. Ideally, we would like to see an agent learning quickly and then stably maintaining scores over the solution threshold of 200 points.\n",
    "\n",
    "Through this methodology, we will be able to explore the advantages and disadvantages of various models and demonstrate how a solution to the problem can be explored through the modfication of features and comparison of models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results ## \n",
    "\n",
    "show the result and interpretation of your experiment. Any iterative improvements summary. demo clips\n",
    "Does it include the results summary, interpretation of experiments and visualization (e.g. performance comparison table, graphs etc)?\n",
    "\n",
    "In this section, I compare the four different models: a random agent, a simple reflex agent, a Q-learning agent, and an approximate reinforcement learning agent.\n",
    "\n",
    "### Random Agent ###\n",
    "\n",
    "As previously mentioned, the random agent selects any of the four actions randomly at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 score: -113.23355557454558\n",
      "Episode 1 score: -199.6521346989494\n",
      "Episode 2 score: -95.84509325825913\n",
      "Episode 3 score: -87.3202301237402\n",
      "Episode 4 score: -92.34389415476073\n",
      "Episode 5 score: -375.41028457777213\n",
      "Episode 6 score: -55.69462635859349\n",
      "Episode 7 score: -159.6638147551339\n",
      "Episode 8 score: -86.24966765370587\n",
      "Episode 9 score: -107.757938912112\n",
      "Episode 10 score: -96.3028985552594\n",
      "Episode 11 score: -489.0343770949044\n",
      "Episode 12 score: -127.37909693198417\n",
      "Episode 13 score: -81.99268365542986\n",
      "Episode 14 score: -123.43821338765292\n",
      "Episode 15 score: -170.36860843023834\n",
      "Episode 16 score: -227.8481241683753\n",
      "Episode 17 score: -229.0242351991031\n",
      "Episode 18 score: -121.59913525218684\n",
      "Episode 19 score: -142.24538719920463\n",
      "Episode 20 score: -127.09021585858281\n",
      "Episode 21 score: -77.5475115931145\n",
      "Episode 22 score: -106.1909038063901\n",
      "Episode 23 score: -113.86986653834505\n",
      "Episode 24 score: -145.81585195014912\n",
      "Episode 25 score: -225.94033089961695\n",
      "Episode 26 score: -135.3251812014649\n",
      "Episode 27 score: -310.45443726273027\n",
      "Episode 28 score: -108.31520559034294\n",
      "Episode 29 score: -102.8662816626549\n",
      "Episode 30 score: -96.34832554140583\n",
      "Episode 31 score: -208.72693846182779\n",
      "Episode 32 score: -161.79190563355513\n",
      "Episode 33 score: -280.017384384562\n",
      "Episode 34 score: -237.17800376733135\n",
      "Episode 35 score: -123.79483811204815\n",
      "Episode 36 score: -97.48108503851135\n",
      "Episode 37 score: 55.894301804714246\n",
      "Episode 38 score: -215.11849370576914\n",
      "Episode 39 score: -156.5774507404315\n",
      "Episode 40 score: -301.26075303238645\n",
      "Episode 41 score: -109.7761306023883\n",
      "Episode 42 score: -113.91576014492853\n",
      "Episode 43 score: -107.79331543916447\n",
      "Episode 44 score: -185.89570153619235\n",
      "Episode 45 score: -139.44303254315685\n",
      "Episode 46 score: -158.86633107234243\n",
      "Episode 47 score: -90.5970065214295\n",
      "Episode 48 score: -409.0971850226492\n",
      "Episode 49 score: -102.60492272017375\n",
      "Episode 50 score: -188.93958341647766\n",
      "Episode 51 score: -92.25804933442933\n",
      "Episode 52 score: -93.18959895734442\n",
      "Episode 53 score: -38.78880888574038\n",
      "Episode 54 score: -172.11705251117746\n",
      "Episode 55 score: -396.9162719507028\n",
      "Episode 56 score: -81.58331076831364\n",
      "Episode 57 score: -146.6392741285507\n",
      "Episode 58 score: -288.9451293067363\n",
      "Episode 59 score: -243.52058686141172\n",
      "Episode 60 score: -285.309366571246\n",
      "Episode 61 score: -112.032790854455\n",
      "Episode 62 score: -160.8969678061316\n",
      "Episode 63 score: -102.56152235373422\n",
      "Episode 64 score: -85.35313282764034\n",
      "Episode 65 score: -220.44053227311502\n",
      "Episode 66 score: -134.47070113326103\n",
      "Episode 67 score: -402.9969379270126\n",
      "Episode 68 score: -345.1586791300342\n",
      "Episode 69 score: -275.7000042378033\n",
      "Episode 70 score: -261.00576918349867\n",
      "Episode 71 score: -101.0904216991479\n",
      "Episode 72 score: -221.7395034363467\n",
      "Episode 73 score: -388.079179852907\n",
      "Episode 74 score: -119.67345193636775\n",
      "Episode 75 score: -87.1236177803341\n",
      "Episode 76 score: -340.0227035598938\n",
      "Episode 77 score: -323.99208353108736\n",
      "Episode 78 score: -297.7400980305148\n",
      "Episode 79 score: -339.3270641549045\n",
      "Episode 80 score: -441.7356888647746\n",
      "Episode 81 score: -131.59641389859004\n",
      "Episode 82 score: -313.7935078870714\n",
      "Episode 83 score: -86.48699037946079\n",
      "Episode 84 score: -281.25568638237826\n",
      "Episode 85 score: -234.3512513646156\n",
      "Episode 86 score: -1.4951995173066877\n",
      "Episode 87 score: -102.67623873141459\n",
      "Episode 88 score: -241.94395175197724\n",
      "Episode 89 score: -414.51529314722274\n",
      "Episode 90 score: -192.3630871800671\n",
      "Episode 91 score: -161.87525033133247\n",
      "Episode 92 score: -120.7412238051064\n",
      "Episode 93 score: -146.95710711384274\n",
      "Episode 94 score: -157.10073198034738\n",
      "Episode 95 score: -104.90059840422009\n",
      "Episode 96 score: -80.04413383183368\n",
      "Episode 97 score: -305.5721719458419\n",
      "Episode 98 score: -81.93076104381771\n",
      "Episode 99 score: -98.56775977588849\n",
      "Episode 100 score: 0.3859430710620444\n",
      "Episode 101 score: -407.9198951728267\n",
      "Episode 102 score: -348.30141673020137\n",
      "Episode 103 score: -180.8688315615422\n",
      "Episode 104 score: -280.9359579614943\n",
      "Episode 105 score: -103.398217855109\n",
      "Episode 106 score: -96.11685627434795\n",
      "Episode 107 score: -205.76955409830356\n",
      "Episode 108 score: -406.9379702102294\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'Array'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/tyler/Projects/cspb3202/rl-project/project.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 82>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tyler/Projects/cspb3202/rl-project/project.ipynb#X41sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m         observation, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tyler/Projects/cspb3202/rl-project/project.ipynb#X41sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m env\u001b[39m.\u001b[39mclose()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tyler/Projects/cspb3202/rl-project/project.ipynb#X41sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m ra_scores \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mArray(scores)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tyler/Projects/cspb3202/rl-project/project.ipynb#X41sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAverage score: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39maverage(ra_scores)\u001b[39m}\u001b[39;00m\u001b[39m; Max score: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmax(ra_scores)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/__init__.py:311\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtesting\u001b[39;00m \u001b[39mimport\u001b[39;00m Tester\n\u001b[1;32m    309\u001b[0m     \u001b[39mreturn\u001b[39;00m Tester\n\u001b[0;32m--> 311\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'Array'"
     ]
    }
   ],
   "source": [
    "# Run and record random agent in lunar lander environment\n",
    "# References: \n",
    "# https://gymnasium.farama.org/\n",
    "# https://gymnasium.farama.org/api/utils/#save-rendering-videos\n",
    "# https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "# https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py\n",
    "\n",
    "# Install dependencies\n",
    "# pip install gymnasium\n",
    "# pip install gymnasium[box2d]\n",
    "# pip install moviepy\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video\n",
    "import numpy as np\n",
    "from IPython.utils.capture import capture_output\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make(\"LunarLander-v2\", \n",
    "               continuous = False,          # Discrete version\n",
    "               gravity = -10.0, \n",
    "               enable_wind = False, \n",
    "               wind_power = 0.0, \n",
    "               turbulence_power = 0.0, \n",
    "               render_mode=\"rgb_array_list\") # Render for machine\n",
    "\n",
    "# Reset environment with random number generator seed for reproducibility\n",
    "observation, info = env.reset(seed = 21)\n",
    "\n",
    "# Accumulator variables\n",
    "score = 0.0\n",
    "scores = []\n",
    "episode = 0\n",
    "step_start_index = 0\n",
    "\n",
    "# Attempt for 10,000 timesteps\n",
    "for step_index in range(10000):\n",
    "\n",
    "    # Get random action from action space\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # Obtain observation, reward, terminated status, truncated status, \n",
    "    # and environment info for given action\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Accumulate reward total \n",
    "    score += reward\n",
    "\n",
    "    # If episode ends\n",
    "    if terminated or truncated:\n",
    "        # Append total to list of reward totals\n",
    "        scores.append(score)\n",
    "\n",
    "        # Report result\n",
    "        print(f\"Episode {episode} score: {score}\")\n",
    "\n",
    "        # Suppress MoviePy stdout\n",
    "        # https://stackoverflow.com/a/35624406/14371011\n",
    "        with capture_output() as captured:\n",
    "            # Save mp4 video of every (k^3)th (i.e., 0, 1, 8, 27, ...)\n",
    "            save_video(env.render(),\n",
    "                    video_folder = \"videos\",\n",
    "                    name_prefix = \"random_agent\",\n",
    "                    fps = env.metadata[\"render_fps\"],\n",
    "                    step_starting_index = step_start_index,\n",
    "                    episode_index = episode)\n",
    "\n",
    "        # Increment episode\n",
    "        episode += 1\n",
    "\n",
    "        # Set starting step index for videos to next step\n",
    "        step_start_index = step_index + 1\n",
    "\n",
    "        # Reset reward total\n",
    "        score = 0\n",
    "        \n",
    "        # Start new episode \n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n",
    "\n",
    "ra_scores = np.array(scores)\n",
    "\n",
    "print(f\"Average score: {np.average(ra_scores)}; Max score: {np.max(ra_scores)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to open video of 64<sup>th</sup> episode](videos/random_agent-episode-64.mp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 41 cubes from https://oeis.org/A000578\n",
    "cubes = [0, 1, 8, 27, 64, 125, 216, 343, 512, 729, 1000, 1331, 1728, 2197, 2744, 3375, 4096, 4913, 5832, 6859, 8000, 9261, 10648, 12167, 13824, 15625, 17576, 19683, 21952, 24389, 27000, 29791, 32768, 35937, 39304, 42875, 46656, 50653, 54872, 59319, 64000]\n",
    "len(cubes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(ra_scores)\n",
    "plt.ylim([-500,0])\n",
    "plt.title(\"Random Agent\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Score\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above graph shows, the random agent does not learn anything and never achieves a positive score on any attempt,as expected.\n",
    "\n",
    "### Simple Reflex Agent ###\n",
    "\n",
    "Next, we examine an agent who can fire its left and right thrusters in reaction to its horizontal distance from the landing pad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple reflex agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Agent ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate Reinforcement Leaning Agent ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion ## \n",
    "\n",
    "Does it include discussion (what went well or not and why), and suggestions for improvements or future work?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggestions for Future Research ##\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References ##\n",
    "\n",
    "Gymnasium. (2022). _Gymnasium documentation._ Farama Foundation. [https://gymnasium.farama.org/](https://gymnasium.farama.org/)\n",
    "\n",
    "Russell, S., & Norvig, R. (2022). Artificial intelligence: A modern approach, (4th ed.). Pearson. \n",
    "\n",
    "Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT Press."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
