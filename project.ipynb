{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSPB 3202 Final Project #\n",
    "\n",
    "Tyler Kinkade, Tyler.Kinkade@colorado.edu    \n",
    "\n",
    "_All rights reserved_\n",
    "\n",
    "GitHub: [https://github.com/jaki9292/rl-project](https://github.com/jaki9292/rl-project)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview ##\n",
    "\n",
    "This write-up reports on a small project to compare the effectiveness of artificial agent algorithms ranging in sophistication from naive to deep reinforcement learning (Russell & Norvig, 2022; Sutton & Barto, 2018) to successfully land a \"lunar lander\" agent in a Gymnasium (2022) model environment.\n",
    "\n",
    "The report is divided into the following sections: approach, results, discussion, and suggestions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach ##\n",
    "\n",
    "This section is divided into the following subsections: environment, models, methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment ###\n",
    "\n",
    "The lunar lander environment (pictured below) simulates rocket trajectory physics with the primary aim of landing a lunar lander on a central landing pad (marked by two flags) by means of turning its three rockets thrusters (left, right, and main) on or off. The agent operating the lander is rewarded for landing slowly, in an upright position, on both legs, on or near the landing pad and penalized otherwise. Fuel is unlimited, but a penalty is given for each time an engine fires. Scores of 200 points or more are considered a solution. \n",
    "\n",
    "The state space is described by an 8-vector comprised of the lander coordinates $(x,y)$, its linear velocities $(v_x,v_y)$ in the $x$ and $y$ directions, its angle $(\\theta)$, its angular velocity $(\\omega)$, and two Boolean variables $(l,r)$ that encode whether the left and right legs are in contact with the ground. The environment has both discrete and continuous versions, but only the discrete version is used here, in order to include simple models in the comparison. The environment gravity, wind power, and turbulence can also be specified. The starting state is the top center of the space with a random force applied to the lander. The termination state occurs when the lander stops moving or moves outside the frame. The action space is comprised of 4 discrete actions: do nothing, fire left thruster, fire main down thruster, or fire right thruster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://gymnasium.farama.org/_images/lunar_lander.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Lunar Lander GIF\n",
    "from IPython.display import Image\n",
    "Image(url= \"https://gymnasium.farama.org/_images/lunar_lander.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# %%bash\n",
    "# !pip3 install gymnasium\n",
    "# !pip3 install gymnasium[box2d]\n",
    "# !pip3 install moviepy\n",
    "# !pip3 install numpy\n",
    "# !pip3 install pandas\n",
    "# !pip3 install matplotlib\n",
    "# !pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment metadata:\n",
      " {'render_modes': ['human', 'rgb_array'], 'render_fps': 50}\n",
      "\n",
      "State space:\n",
      " Box([-90.        -90.         -5.         -5.         -3.1415927  -5.\n",
      "  -0.         -0.       ], [90.        90.         5.         5.         3.1415927  5.\n",
      "  1.         1.       ], (8,), float32)\n",
      "\n",
      "Action space:\n",
      " Discrete(4)\n",
      "\n",
      "Starting state:\n",
      " [ 0.00786695  1.4032904   0.796822   -0.33911794 -0.00910905 -0.18049194\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Display environment parameters\n",
    "# References: \n",
    "# https://gymnasium.farama.org/api/env/\n",
    "# https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make(\"LunarLander-v2\", \n",
    "               continuous = False,      # Discrete version\n",
    "               gravity = -10.0, \n",
    "               enable_wind = False, \n",
    "               wind_power = 0.0, \n",
    "               turbulence_power = 0.0, \n",
    "               render_mode=\"rgb_array\") # Render for machine\n",
    "\n",
    "# Reset environment with random number generator seed for reproducibility\n",
    "state, info = env.reset(seed = 21)\n",
    "\n",
    "print(\"Environment metadata:\\n\", env.metadata)\n",
    "print(\"\\nState space:\\n\", env.observation_space)\n",
    "print(\"\\nAction space:\\n\", env.action_space)\n",
    "print(\"\\nStarting state:\\n\", state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below summarizes the descriptive statistics for 100 randomly sampled initial states for this environment. We can see that the initial linear and angular velocities vary more than the position and angular orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>vx</th>\n",
       "      <th>vy</th>\n",
       "      <th>angle</th>\n",
       "      <th>ang_vel</th>\n",
       "      <th>left_leg</th>\n",
       "      <th>right_leg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.000202</td>\n",
       "      <td>1.410030</td>\n",
       "      <td>-0.020460</td>\n",
       "      <td>-0.039560</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.004635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.004445</td>\n",
       "      <td>0.007046</td>\n",
       "      <td>0.450270</td>\n",
       "      <td>0.313165</td>\n",
       "      <td>0.005151</td>\n",
       "      <td>0.101993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.007806</td>\n",
       "      <td>1.398572</td>\n",
       "      <td>-0.790636</td>\n",
       "      <td>-0.548822</td>\n",
       "      <td>-0.009096</td>\n",
       "      <td>-0.180236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.003390</td>\n",
       "      <td>1.404051</td>\n",
       "      <td>-0.343335</td>\n",
       "      <td>-0.305285</td>\n",
       "      <td>-0.003875</td>\n",
       "      <td>-0.076865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.000128</td>\n",
       "      <td>1.409595</td>\n",
       "      <td>-0.012973</td>\n",
       "      <td>-0.058933</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.002939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.003350</td>\n",
       "      <td>1.416666</td>\n",
       "      <td>0.339339</td>\n",
       "      <td>0.255355</td>\n",
       "      <td>0.003934</td>\n",
       "      <td>0.077771</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.007856</td>\n",
       "      <td>1.421770</td>\n",
       "      <td>0.795693</td>\n",
       "      <td>0.482209</td>\n",
       "      <td>0.009052</td>\n",
       "      <td>0.179091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                x           y          vx          vy       angle     ang_vel   \n",
       "count  100.000000  100.000000  100.000000  100.000000  100.000000  100.000000  \\\n",
       "mean    -0.000202    1.410030   -0.020460   -0.039560    0.000241    0.004635   \n",
       "std      0.004445    0.007046    0.450270    0.313165    0.005151    0.101993   \n",
       "min     -0.007806    1.398572   -0.790636   -0.548822   -0.009096   -0.180236   \n",
       "25%     -0.003390    1.404051   -0.343335   -0.305285   -0.003875   -0.076865   \n",
       "50%     -0.000128    1.409595   -0.012973   -0.058933    0.000155    0.002939   \n",
       "75%      0.003350    1.416666    0.339339    0.255355    0.003934    0.077771   \n",
       "max      0.007856    1.421770    0.795693    0.482209    0.009052    0.179091   \n",
       "\n",
       "       left_leg  right_leg  \n",
       "count     100.0      100.0  \n",
       "mean        0.0        0.0  \n",
       "std         0.0        0.0  \n",
       "min         0.0        0.0  \n",
       "25%         0.0        0.0  \n",
       "50%         0.0        0.0  \n",
       "75%         0.0        0.0  \n",
       "max         0.0        0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Determine average starting state from 100 random samples\n",
    "starting_states = []\n",
    "for _ in range(100):\n",
    "    starting_state, info = env.reset()\n",
    "    starting_states.append(starting_state)\n",
    "\n",
    "features = ['x','y','vx','vy','angle','ang_vel','left_leg','right_leg']\n",
    "starting_states_df = pd.DataFrame(starting_states, columns = features)\n",
    "starting_states_df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models ###\n",
    "Four models are compared in this project: an agentless lunar lander, a random agent, a simple reflex agent, and a deep Q-network agent. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agentless Lunar Lander ####\n",
    "An agentless lunar lander takes no action and is at the complete mercy of its environment. Because the starting state is directly above the landing pad, this might not be as terrible a strategy as it initially seems. The lander will simply fall to the ground in a trajectory determined by the physics of the environmental forces and the initial force placed upon it.\n",
    "\n",
    "#### Random Agent ####\n",
    "\n",
    "A random agent selects any of the possible actions uniformly randomly at each step without regard to the environment. The animation above is an example of such an agent and appears to be a poor strategy. Is this a worse strategy than an agentless lander though?\n",
    "\n",
    "#### Simple Reflex Agent ####\n",
    "\n",
    "A simple reflex agent can only react in a fixed way to sensory input but does not retain any memory of the past and, as a result, cannot learn (Russell & Norvig, 2022). As explained in the environment section, the initial lunar lander position is generally centered above the landing pad, but there is wide variation in initial linear and angular velocity. Thus, the primary task is counteracting any horizontal velocity while at the same time, avoiding turning the ship on its side. Therefore, there is a balance to be achieved, and the programmer must select appropriate threshold constants for the algorithm. Depending on the algorithm and constants, this could potentially be a better strategy than doing nothing or acting randomly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Q-Network Agent ####\n",
    "\n",
    "A deep Q-network (DQN; Mnih et al., 2015, 2016) agent is a deep-learning neural network approximation of a Q-learning agent (Russell & Norvig, 2022; Sutton & Barto, 2018). A Q-learning agent alternates between learning through exploration and exploiting what it has learned by choosing the action with maximal value for each state. Although this is obviously better than the relatively ignorant reflex agent, Q-learning agents can quickly become computationally expensive and slow to learn in large and complex environments (Russell & Norvig, 2022; Sutton & Barto, 2018). \n",
    "\n",
    "Approximate Q-learning agents address this problem by approximating the Q-value of actions in particular states with a weighted feature function based on states and actions (Russell & Norvig, 2022; Sutton & Barto, 2018). However, in the environment in this project, we only have access to the current features and do not know how any particular action will influence those features. \n",
    "\n",
    "Fortunately, the universal approximation theorem tells us that we can approximate any function with a neural network of at least two layers  (Russell & Norvig, 2022). Thus, we can approximate a Q-value function with a DQN by recording a log of previous attempts (i.e., replay buffer) to train the network parameters (Paske & Towers, 2023). In other words, the agent remembers and learns from its experiences.\n",
    "\n",
    "For this project, the DQN consists of three fully-connected neural network layers. A non-linear activation function is placed between the layers to prevent the model from collapsing into a one-layer linear model. Here, a rectified linear unit (ReLU) function is selected for this because of its computational simplicity and its consistent, demonstrated effectiveness in previous studies (Goodfellow et al., 2016; Karpathy, 2016). The input to the network are the eight state features and the output is a vector of approximate Q-values for each of the four possible actions given the input state. \n",
    "\n",
    "It is worth noting that image-based convolutional neural networks are frequently used for DQNs (e.g., Mnih et al., 2015). However, I am interested here in solving the lunar landing problem from the perspective of the sensory information available to an agent within the lunar lander, rather than an outside observer.\n",
    "\n",
    "Another design choice is how often the agent should explore versus how often it should exploit what it has learned thus far. For this project, I will use an epsilon greedy policy which prioritizes learning at the beginning but gradually shifts to prioritizing achieving the highest rewards by selecting the policy it has learned thus far.\n",
    "\n",
    "To train the model, I will use the [Huber loss](https://en.wikipedia.org/wiki/Huber_loss) function (Huber, 1964), which is more robust to noisy data than mean-squared error for training the model (Paske & Towers, 2023).For optimization, I will use the [Adam optimization algorithm](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/). Unlike the\n",
    "[stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (SGD), Adam optimization maintains and adapts separate learning rates for each parameter (Brownlee, 2017).\n",
    "\n",
    "It is expected that the DQN model will perform better than the first three; however, it is still possible that some difficulties might arise in creating a suitable neural network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods ###\n",
    "\n",
    "Each model will be run for 1000 episodes and their scores over each of the episodes will be plotted and compared. Visualizing each model's performance over successive episodes permits comparison of how each agent progressed in learning how to solve the model. As Machado and his colleagues (2018) point out, this is preferable to single metrics, such as maximum or average scores which can conceal important distinctions. For example, we can discover whether an agent achieved a maximum only once, but performed poorly most of the time. Similarly, an average score can conceal unstable, fluctuating performance. Ideally, we would like to see an agent learning quickly and then stably maintaining high scores above the solution threshold of 200 points.\n",
    "\n",
    "Applying this methodology, we will be able to explore the advantages and disadvantages of the various models and also demonstrate how a solution to the problem can be explored through the modfication of features and comparison of models.\n",
    "\n",
    "In the following code blocks, I construct the Python classes required to encode each of the agents."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agentless Lunar Lander ####\n",
    "An agentless lunar lander which never fires any of its thrusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define general lunar lander class\n",
    "# Adapted from: \n",
    "# CSPB 3202 Gym tutorial (https://applied.cs.colorado.edu/mod/folder/view.php?id=43136)\n",
    "# https://gymnasium.farama.org/\n",
    "# https://gymnasium.farama.org/api/utils/#save-rendering-videos\n",
    "# https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "# https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py\n",
    "# https://gymnasium.farama.org/tutorials/training_agents/FrozenLake_tuto/\n",
    "\n",
    "from gymnasium.utils.save_video import save_video\n",
    "import numpy as np\n",
    "from IPython.utils.capture import capture_output\n",
    "\n",
    "# Action space\n",
    "NOOP, LEFT, MAIN, RIGHT = 0, 1, 2, 3\n",
    "\n",
    "class LunarLander:\n",
    "    '''\n",
    "    General lunar lander class \n",
    "    subject entirely to its environment\n",
    "    '''   \n",
    "    def __init__(self, name = \"agentless\"):\n",
    "        '''\n",
    "        Initialize agent\n",
    "        '''\n",
    "        self.name = name\n",
    "        self.env = gym.make(\"LunarLander-v2\", \n",
    "               continuous = False,           # Discrete version\n",
    "               gravity = -10.0, \n",
    "               enable_wind = False, \n",
    "               wind_power = 0.0, \n",
    "               turbulence_power = 0.0, \n",
    "               render_mode=\"rgb_array_list\") # Render for machine\n",
    "        \n",
    "        # Reset environment with random number generator seed for reproducibility\n",
    "        self.state, self.info = self.env.reset(seed = 21)\n",
    "        self.features_size = len(self.state)\n",
    "        self.actions_size = env.action_space.n\n",
    "        self.scores = []\n",
    "        self.steps = []\n",
    "        self.policy = []\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        '''\n",
    "        Select action given state\n",
    "        '''\n",
    "        # Do nothing\n",
    "        return NOOP\n",
    "\n",
    "    def episode(self):\n",
    "        '''\n",
    "        Complete episode in given environment\n",
    "        Return score, steps count, and actions log\n",
    "        '''\n",
    "        # Initialize rewards total\n",
    "        score = 0.0\n",
    "\n",
    "        # Initialize steps count\n",
    "        step_count = 0\n",
    "\n",
    "        # Initialize actions list\n",
    "        actions = []\n",
    "\n",
    "        # Start new episode  \n",
    "        state, info = self.env.reset()\n",
    "\n",
    "        # Until end of episode\n",
    "        while True:\n",
    "            # Get action based on current state\n",
    "            action = self.select_action(state)\n",
    "\n",
    "            # Obtain next state, reward, terminated status, truncated status, \n",
    "            # and environment info for given action\n",
    "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            \n",
    "            # Accumulate reward total\n",
    "            score += reward\n",
    "\n",
    "            # Increment steps count\n",
    "            step_count += 1\n",
    "\n",
    "            # Record action\n",
    "            actions.append(action)\n",
    "\n",
    "            # If end of episode\n",
    "            if terminated or truncated:\n",
    "                break  # out of loop\n",
    "            \n",
    "            # Advance to next state \n",
    "            state = next_state\n",
    "\n",
    "        return score, step_count, actions\n",
    "    \n",
    "    def iterate(self, n = 100, verbose = False, video = False):\n",
    "        '''\n",
    "        Attempt task for n episodes\n",
    "        '''\n",
    "        # Initialize logs\n",
    "        scores = []\n",
    "        steps = []\n",
    "        policy = []\n",
    "\n",
    "        video_step_start = 0\n",
    "        \n",
    "        # Repeat n times\n",
    "        for episode in range(n):\n",
    "            # Complete one episode\n",
    "            score, step_count, actions = self.episode()\n",
    "            \n",
    "            # Log scores, steps, policy\n",
    "            scores.append(score)\n",
    "            steps.append(step_count)\n",
    "            policy.append(actions)\n",
    "\n",
    "            # Update object attributes\n",
    "            self.scores = scores\n",
    "            self.steps = steps\n",
    "            self.policy = policy\n",
    "\n",
    "            if verbose:\n",
    "                # Report result\n",
    "                print(f\"Episode {episode} score: {score}\")\n",
    "\n",
    "            if video:\n",
    "                # Suppress MoviePy stdout\n",
    "                # https://stackoverflow.com/a/35624406/14371011\n",
    "                with capture_output() as captured:\n",
    "                    # Save mp4 of every (k^3)th episode (0, 1, 8, 27, ...)\n",
    "                    save_video(self.env.render(),\n",
    "                               video_folder = \"videos\",\n",
    "                               name_prefix = self.name,\n",
    "                               fps = env.metadata[\"render_fps\"],\n",
    "                               step_starting_index = video_step_start,\n",
    "                               episode_index = episode)\n",
    "\n",
    "            # Advance starting step index for videos\n",
    "            video_step_start = step_count + 1           \n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def score_stats(self, verbose = False):\n",
    "        '''\n",
    "        Return mean, max, and min of scores if they exist \n",
    "        '''\n",
    "        # If scores exist\n",
    "        if (len(self.scores) > 0):\n",
    "            # Calculate average, max, and min\n",
    "            average, max, min = np.average(self.scores), np.max(self.scores), np.min(self.scores)\n",
    "            \n",
    "            if verbose:\n",
    "                # Report average, max, and min\n",
    "                print(f\"\\nAverage score: {average}; Max: {max}; Min: {min}\")\n",
    "            return average, max, min\n",
    "        else: \n",
    "            print(\"No scores have been calculated\")\n",
    "            return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reference, the first 41 cubes (from https://oeis.org/A000578) are:\n",
    "cubes = [0, 1, 8, 27, 64, 125, 216, 343, 512, 729, 1000, 1331, 1728, 2197, 2744, 3375, 4096, 4913, 5832, 6859, 8000, 9261, 10648, 12167, 13824, 15625, 17576, 19683, 21952, 24389, 27000, 29791, 32768, 35937, 39304, 42875, 46656, 50653, 54872, 59319, 64000]\n",
    "len(cubes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Agent ####\n",
    "An agent which selects any of the four possible actions randomly at each time step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define random agent class\n",
    "\n",
    "class RandomAgent(LunarLander):\n",
    "    '''\n",
    "    Inherits from lunar lander class\n",
    "    and acts randomly\n",
    "    '''\n",
    "    def __init__(self, name = \"random_agent\"):\n",
    "        LunarLander.__init__(self, name)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        '''\n",
    "        Randomly select action\n",
    "        '''\n",
    "        # Randomly select action from action space\n",
    "        action = self.env.action_space.sample()\n",
    "\n",
    "        return action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Reflex Agent ####\n",
    "\n",
    "This is an agent which can fire its side thrusters in reaction to its current horizontal and angular velocity in relation to threshold constants chosen by the programmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflexAgent(LunarLander):\n",
    "    '''\n",
    "    Inherits from lunar lander class, but \n",
    "    selects actions according to horizontal \n",
    "    and angular position and angular velocity\n",
    "    '''\n",
    "    def __init__(self, name = \"reflex_agent\"):\n",
    "        LunarLander.__init__(self, name)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        '''\n",
    "        Select action according to horizontal and\n",
    "        angular position and angular velocity\n",
    "        '''\n",
    "        # Set tolerances\n",
    "        horiz_vel_tolerance = 0.4\n",
    "        ang_vel_tolerance = 0.001\n",
    "\n",
    "        # Horizontal velocity\n",
    "        vx = state[3]\n",
    "\n",
    "        # Angular velocity\n",
    "        ang_vel = state[5]\n",
    "\n",
    "        # If leftward horizontal velocity\n",
    "        if ((vx < -horiz_vel_tolerance) and\n",
    "            # and clockwise velocity\n",
    "            (ang_vel > ang_vel_tolerance)):\n",
    "\n",
    "            # Then, fire thruster to move right\n",
    "            action = RIGHT\n",
    "        \n",
    "        # If rightward horizontal velocity\n",
    "        elif ((vx > horiz_vel_tolerance) and\n",
    "              # If counter-clockwise velocity\n",
    "              (ang_vel < -ang_vel_tolerance)):\n",
    "            \n",
    "            # Then, fire thruster to move left\n",
    "            action = LEFT\n",
    "\n",
    "        else:\n",
    "            # Do nothing\n",
    "            action = NOOP\n",
    "        \n",
    "        return action "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Q-Network Agent ####\n",
    "\n",
    "A deep Q-network agent has the capacity to remember and learn from its previous experiences using a neural network.\n",
    "\n",
    "##### Replay Memory and Deep Q-Network #####\n",
    "First, we must define a buffer to remember previous experiences and a neural network for the agent to use for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Replay Memory Buffer and Deep Q-Network\n",
    "# Adapted from: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "# Additional references:\n",
    "# https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html#building-models-with-pytorch\n",
    "# https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\",device)\n",
    "\n",
    "# Named tuple to encode a single state transition\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# Buffer class to hold and randomly sample recent transitions\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        '''\n",
    "        Initialize buffer with empty double-ended queue\n",
    "        of size \"capacity\"\n",
    "        '''\n",
    "        self.memory = deque([], maxlen = capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        '''\n",
    "        Add a transition to buffer\n",
    "        '''\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''\n",
    "        Randomly sample batch of transitions from buffer\n",
    "        '''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns number of transitions in buffer\n",
    "        '''\n",
    "        return len(self.memory)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Deep Q-Network\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, features_size, actions_size):\n",
    "        '''\n",
    "        Initialize neural network\n",
    "        '''\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # First fully-connected layer: \n",
    "        # Input dimensions: 1 x no. of state feature\n",
    "        # Output dimensions: 1 x 128\n",
    "        self.fc1 = nn.Linear(features_size, 128)\n",
    "\n",
    "        # Second fully-connected layer: \n",
    "        # Input: 1 x 128; Output: 1 x 128\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "\n",
    "        # Final fully-connected layer: \n",
    "        # Input: 1 x 128; \n",
    "        # Output: 1 x no. of possible actions\n",
    "        self.fc3 = nn.Linear(128, actions_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        '''\n",
    "        Forward propagation function\n",
    "        Input: State features (e.g., position, velocity, ...)\n",
    "        Output: tensor of action values\n",
    "        '''\n",
    "        # First layer followed by ReLU activation function\n",
    "        z = F.relu(self.fc1(state))\n",
    "        \n",
    "        # Second layer followed by ReLU activation function\n",
    "        z = F.relu(self.fc2(z))\n",
    "        \n",
    "        # Final output layer\n",
    "        return self.fc3(z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DQN Agent ####\n",
    "Now we can define the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from itertools import count\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(LunarLander):\n",
    "    '''\n",
    "    Inherits from lunar lander class, but \n",
    "    iteratively learns from experience to approximate \n",
    "    Q-values from state features with a neural network\n",
    "    '''\n",
    "    def __init__(self, name = \"dqn_agent\", alpha = 0.5, epsilon = 0.1):\n",
    "        LunarLander.__init__(self, name)\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def update(self, state, action, reward, new_state):\n",
    "        '''\n",
    "        Updates weights\n",
    "        '''\n",
    "        return None\n",
    "\n",
    "    def select_action(self, state):\n",
    "        '''\n",
    "        Select action\n",
    "        '''\n",
    "                \n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results ## \n",
    "\n",
    "In this section, I compare the five different models: an agentless lunar lander, a random agent, a simple reflex agent, and a DQN agent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agentless Lunar Lander ###\n",
    "First, we consider the agentless lunar lander which never fires any of its thrusters and is therefore subject entirely to its environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate agentless lunar lander\n",
    "non_agent = LunarLander()\n",
    "\n",
    "# Number of episodes to iterate over\n",
    "episodes = 100\n",
    "\n",
    "# Iterate over 100 episodes\n",
    "non_scores = non_agent.iterate(episodes, verbose = False, video = True)\n",
    "\n",
    "# Print average, max, min\n",
    "average, max, min = non_agent.score_stats(verbose = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to the video of the agentless lander in the 65<sup>th</sup> episode](videos/agentless-episode-64.mp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot agentless lunar lander scores over all episodes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(non_scores, label = \"Agentless Lander\")\n",
    "plt.axhline(y = 200, color = \"r\", linestyle = \"dashed\", label = \"Solution Threshold\")  \n",
    "plt.xlim([0,episodes])\n",
    "plt.ylim([-600,300])\n",
    "plt.title(\"Agentless Lunar Lander\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above graph, we can see that an agentless lunar lander is not sufficient to solve this problem. In the video linked above, we can see the lunar lander simply falling in a slightly curved trajectory toward the ground."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent ###\n",
    "\n",
    "Next, we consider the random agent which selects an action randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate lunar lander random agent\n",
    "rand_agent = RandomAgent()\n",
    "\n",
    "# Iterate over episodes\n",
    "rand_scores = rand_agent.iterate(episodes, verbose = False, video = True)\n",
    "\n",
    "# Print average, max, min\n",
    "average, max, min = rand_agent.score_stats(verbose = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to the video of the random agent in the 65<sup>th</sup> episode](videos/random_agent-episode-64.mp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot agentless lander and random agent scores over all episodes\n",
    "\n",
    "plt.plot(non_scores, label = \"Agentless Lander\")\n",
    "plt.plot(rand_scores, label = \"Random Agent\")\n",
    "plt.axhline(y = 200, color = \"r\", linestyle = \"dashed\", label = \"Solution Threshold\")  \n",
    "plt.xlim([0,episodes])\n",
    "plt.ylim([-600,300])\n",
    "plt.title(\"Random Agent vs. Agentless Lunar Lander\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above graph shows, doing nothing is better than acting randomly in this environment. The agentless lunar lander is relatively more consistent and successful compared to the random agent and also has a slightly higher average score. Obviously, neither learn anything nor come remotely close to consistently reaching the solution threshold, as expected."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Reflex Agent ###\n",
    "\n",
    "Next, we examine the agent which can reflexively fire its side engines in reaction to its current horizontal and angular velocity mediated by pre-selected threshold constants. Choosing ideal threshold constants requires tedious trial and error on the part of the programmer, rather than the agent. Thus, only the programmer is learning in this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate lunar lander reflex agent\n",
    "rflx_agent = ReflexAgent()\n",
    "\n",
    "# Iterate over episodes\n",
    "rflx_scores = rflx_agent.iterate(episodes, verbose = False, video = True)\n",
    "\n",
    "# Print average max, min\n",
    "average, max, min = rflx_agent.score_stats(verbose = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to the video of the reflex agent in the 65<sup>th</sup> episode](videos/reflex_agent-episode-64.mp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot agentless, random, and reflex lander scores over all episodes\n",
    "\n",
    "plt.plot(non_scores, label = \"Agentless Lander\")\n",
    "plt.plot(rand_scores, label = \"Random Agent\")\n",
    "plt.plot(rflx_scores, label = \"Reflex Agent\")\n",
    "plt.axhline(y = 200, color = \"r\", linestyle = \"dashed\", label = \"Solution Threshold\")  \n",
    "plt.xlim([0,episodes])\n",
    "plt.ylim([-600,300])\n",
    "plt.title(\"Random Agent, Reflex Agent, and Agentless Lander\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above graph, we can see that the reflex agent is only slightly more consistent and successful than the agentless lunar lander but still not even close to solving the problem. Additionally, it is clearly not learning, as expected. Landing a lunar rover appears to be more difficult than firing the left and right thrusters based on horizontal and angular velocity. It might be possible to achieve a higher score by comparing various algorithms and parameters through trial and error, but this would be a tedious process, so we would like to find a more expedient way to find the best policy to follow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Network Agent ###\n",
    "\n",
    "Next, we examine the performance of a remembering, learning deep Q-network agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate lunar lander DQN agent\n",
    "dqn_agent = DQNAgent()\n",
    "\n",
    "# Iterate over episodes\n",
    "dqn_scores = dqn_agent.iterate(episodes, verbose = False, video = True)\n",
    "\n",
    "# Print average, max, min\n",
    "average, max, min = dqn_agent.score_stats(verbose = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to the video of the DQN agent in the 65<sup>th</sup> episode](videos/dqn_agent-episode-64.mp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot agentless, random, reflex, and DQN lander scores over all episodes\n",
    "\n",
    "plt.plot(non_scores, label = \"Agentless Lander\")\n",
    "plt.plot(rand_scores, label = \"Random Agent\")\n",
    "plt.plot(rflx_scores, label = \"Reflex Agent\")\n",
    "plt.plot(dqn_scores, label = \"DQN Agent\")\n",
    "plt.axhline(y = 200, color = \"r\", linestyle = \"dashed\", label = \"Solution Threshold\")  \n",
    "plt.xlim([0,episodes])\n",
    "plt.ylim([-600,300])\n",
    "plt.title(\"All-Model Comparison\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion ## \n",
    "\n",
    "Does it include discussion (what went well or not and why), and suggestions for improvements or future work?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggestions for Future Research ##\n",
    "\n",
    "Stochasticity: frame skipping, sticky actions that repeat with a given probability (Macado et al., 2018)\n",
    "\n",
    "Transfer learning\n",
    "\n",
    "Convolutional network\n",
    "\n",
    "Arcade Learning Environment (Bellemare, 2023; Bellemare et al., 2013; Machado et al., 2018) simulating classic Atari 2600 games (AtariAge, 2023)\n",
    "\n",
    "_AtariAge._ (2023). [Website]. [https://atariage.com](https://atariage.com)\n",
    "\n",
    "Bellemare, M. G. (2023). Arcade Learning Environment. [https://github.com/mgbellemare/Arcade-Learning-Environment](https://github.com/mgbellemare/Arcade-Learning-Environment)\n",
    "\n",
    "Bellemare, M. G., Naddaf, Y., Veness, J., & Bowling, M. (2013). The Arcade Learning Environment: An evaluation platform for general agents. _Journal of Artificial Intelligence Research, 47,_ 253-279.\n",
    "[https://doi.org/10.1613/jair.3912](https://doi.org/10.1613/jair.3912)\n",
    " \n",
    "Finally, it is only right to ask: What is the value of crashing a virtual lunar lander model thousands of times in order to eventually land successfully? The answer is it is better to train a far less-expensive virtual model and then transfer that learning to a real-world environment. Thus, the long-term aim is to develop the ability to transfer this virtual knowledge to a real-world environment.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References ##\n",
    "\n",
    "Brownlee, J. (2017, July 3). Gentle introduction to the Adam optimization algorithm for deep learning. Machine Learning Mastery. [https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
    "\n",
    "_Gymnasium._ (2022). Farama Foundation. [https://gymnasium.farama.org/](https://gymnasium.farama.org/)\n",
    "\n",
    "Goodfellow, I., Bengio, Y., & Courville, A. (2016). _Deep learning._ MIT Press. [http://www.deeplearningbook.org](http://www.deeplearningbook.org)\n",
    "\n",
    "Heintz, B. (2021, April 17). _Building models with Pytorch._ Pytorch. [https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html#building-models-with-pytorch](https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html#building-models-with-pytorch)\n",
    "\n",
    "Huber, P. J. (1964). Robust estimation of a location parameter. _Annals of Statistics, 53_(1), 73–101. [https//doi.org/10.1214/aoms/1177703732](https//doi.org/10.1214/aoms/1177703732)\n",
    "\n",
    "Karpathy, A. (2016). _Convolutional neural networks for visual recognition._ Stanford University. [https://cs231n.github.io/convolutional-networks/](https://cs231n.github.io/convolutional-networks/)\n",
    "\n",
    "Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., & Bowling, M. (2018). Revisiting the Arcade Learning Environment: Evaluation protocols and open problems for general agents. _Journal of Artificial Intelligence Research, 61,_ 523–562. [https://doi.org/10.1613/jair.5699](https://doi.org/10.1613/jair.5699)\n",
    "\n",
    "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., & Hassabis, D. (2015). Human-level control through deep reinforcement learning. _Nature, 518_, 529–533. [https://doi.org/10.1038/nature14236](https://doi.org/10.1038/nature14236)\n",
    "\n",
    "Matplotlib. (2023). _API reference._ [https://matplotlib.org/stable/api/index](https://matplotlib.org/stable/api/index)\n",
    "\n",
    "Mnih, V., Puigdomènech Badia, A., Mirza, M., Graves, A., Harley, T., Lillicrap, T. P., Silver, D., & Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. _Proceedings of the 33rd International Conference on Machine Learning, 48,_ 1928–1937. [https://proceedings.mlr.press/v48/mniha16.html](https://proceedings.mlr.press/v48/mniha16.html)\n",
    "\n",
    "Paske, A., & Towers, M. (2023). _Reinforcement learning (DQN) tutorial._ Pytorch. [https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    "\n",
    "Pytorch. (2023). _Pytorch documentation._ [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)\n",
    "\n",
    "Russell, S., & Norvig, R. (2022). Artificial intelligence: A modern approach, (4th ed.). Pearson. \n",
    "\n",
    "Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT Press."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
