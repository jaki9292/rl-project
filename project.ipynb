{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSPB 3202 Final Project #\n",
    "\n",
    "Tyler Kinkade, jaki9292@colorado.edu\n",
    "\n",
    "GitHub: [https://github.com/jaki9292/rl-project](https://github.com/jaki9292/rl-project)\n",
    "\n",
    "## Overview ##\n",
    "\n",
    "This write-up reports on a small project to train and test a reinforcement learning algorithm (Russell & Norvig, 2022; Sutton & Barto, 2018) with the Gymnasium (2022) Python software package. \n",
    "\n",
    "This report is divided into the following sections: approach, results, discussion, and suggestions for future research.\n",
    "\n",
    "## Approach ##\n",
    "\n",
    "This section is divided into the following subsections: environment and game rules, models, methods and purpose, and problem solving procedure.\n",
    "\n",
    "### Environment and Game Rules ###\n",
    "\n",
    "Does it explain how the environment works and what the game rules are?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lunar Lander Animated gif](lunar_lander.gif)\n",
    "\n",
    "Source: https://gymnasium.farama.org/_images/lunar_lander.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and record random agent in lunar lander environment\n",
    "# References: \n",
    "# https://gymnasium.farama.org/\n",
    "# https://gymnasium.farama.org/api/utils/#save-rendering-videos\n",
    "# https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "# https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py\n",
    "\n",
    "# Install dependencies\n",
    "# pip install gymnasium\n",
    "# pip install gymnasium[box2d]\n",
    "# pip install moviepy\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video\n",
    "from IPython.utils.capture import capture_output\n",
    "\n",
    "# Initialize environment\n",
    "env1 = gym.make(\"LunarLander-v2\", \n",
    "               continuous = False,          # Discrete version\n",
    "               gravity = -10.0, \n",
    "               enable_wind = False, \n",
    "               wind_power = 0.0, \n",
    "               turbulence_power = 0.0, \n",
    "               render_mode=\"rgb_array_list\") # Render for machine\n",
    "\n",
    "# Reset environment with random number generator seed for reproducibility\n",
    "observation, info = env1.reset(seed = 21)\n",
    "\n",
    "# Accumulator variables\n",
    "reward_total = 0.0\n",
    "reward_totals = []\n",
    "episode = 0\n",
    "step_start_index = 0\n",
    "\n",
    "# Attempt for 1000 timesteps\n",
    "for step_index in range(1000):\n",
    "\n",
    "    # Get random action from action space\n",
    "    action = env1.action_space.sample()\n",
    "\n",
    "    # Obtain observation, reward, terminated status, truncated status, \n",
    "    # and environment info for given action\n",
    "    observation, reward, terminated, truncated, info = env1.step(action)\n",
    "\n",
    "    # Accumulate reward total \n",
    "    reward_total += reward\n",
    "\n",
    "    # If episode ends\n",
    "    if terminated or truncated:\n",
    "        # Append total to list of reward totals\n",
    "        reward_totals.append(reward_total)\n",
    "\n",
    "        # Report result\n",
    "        print(f\"Episode {episode} total rewards: {reward_total}\")\n",
    "\n",
    "        # Suppress moviepy stdout\n",
    "        # https://stackoverflow.com/a/35624406/14371011\n",
    "        with capture_output() as captured:\n",
    "            # Save mp4 video of every (k^3)th (i.e., 0, 1, 8, 27, ...)\n",
    "            save_video(env1.render(),\n",
    "                    video_folder = \"videos\",\n",
    "                    name_prefix = \"random_agent\",\n",
    "                    fps = env1.metadata[\"render_fps\"],\n",
    "                    step_starting_index = step_start_index,\n",
    "                    episode_index = episode)\n",
    "\n",
    "        # Increment episode\n",
    "        episode += 1\n",
    "\n",
    "        # Set starting step index for videos to next step\n",
    "        step_start_index = step_index + 1\n",
    "\n",
    "        # Reset reward total\n",
    "        reward_total = 0\n",
    "        \n",
    "        # Start new episode \n",
    "        observation, info = env1.reset()\n",
    "\n",
    "env1.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to open video of first episode](videos/random_agent-episode-0.mp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(reward_totals)\n",
    "plt.title(\"Random Agent\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models ###\n",
    "\n",
    "Does it explain clearly the model(s) of choices, the methods and purpose of tests and experiments?\n",
    "\n",
    "Approximate reinforcement learning (Russell & Norvig, 2022; Sutton & Barto, 2018)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods and Purpose ###\n",
    "\n",
    "methods and purpose of tests and experiments\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Solving Procedure ###\n",
    "\n",
    "Does it show problem solving procedure- e.g. how the author solved and improved when an algorithm doesn't work well. Note that it's not about debugging or programming/implementation, but about when a correctly implemented algorithm wasn't enough for the problem and the author had to modify/add some features or techniques, or compare with another model, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results ## \n",
    "\n",
    "show the result and interpretation of your experiment. Any iterative improvements summary.\n",
    "\n",
    "demo clips\n",
    "\n",
    "Does it include the results summary, interpretation of experiments and visualization (e.g. performance comparison table, graphs etc)?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion ## \n",
    "\n",
    "Does it include discussion (what went well or not and why), and suggestions for improvements or future work?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggestions for Future Research ##\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References ##\n",
    "\n",
    "Gymnasium. (2022). _Gymnasium documentation._ Farama Foundation. [https://gymnasium.farama.org/](https://gymnasium.farama.org/)\n",
    "\n",
    "Russell, S., & Norvig, R. (2022). Artificial intelligence: A modern approach, (4th ed.). Pearson. \n",
    "\n",
    "Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT Press."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
