{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSPB 3202 Final Project #\n",
    "\n",
    "Tyler Kinkade, Tyler.Kinkade@colorado.edu    \n",
    "\n",
    "_All rights reserved_\n",
    "\n",
    "GitHub: [https://github.com/jaki9292/rl-project](https://github.com/jaki9292/rl-project)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview ##\n",
    "\n",
    "This write-up reports on a small project to compare the effectiveness of artificial agent algorithms ranging in sophistication from naive to deep reinforcement learning (Russell & Norvig, 2022; Sutton & Barto, 2018) to successfully land a \"lunar lander\" agent in a Gymnasium (2022) model environment.\n",
    "\n",
    "The report is divided into the following sections: approach, results, discussion, and suggestions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach ##\n",
    "\n",
    "This section is divided into the following subsections: environment, models, methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment ###\n",
    "\n",
    "The lunar lander environment (pictured below) simulates rocket trajectory physics with the primary aim of landing a lunar lander on a central landing pad (marked by two flags) by means of turning its three rockets thrusters (left, right, and main) on or off. The agent operating the lander is rewarded for landing slowly, in an upright position, on both legs, on or near the landing pad and penalized otherwise. Fuel is unlimited, but a penalty is given for each time an engine fires. Scores of 200 points or more are considered a solution. \n",
    "\n",
    "The state space is described by an 8-vector comprised of the lander coordinates $(x,y)$, its linear velocities $(v_x,v_y)$ in the $x$ and $y$ directions, its angle $(\\theta)$, its angular velocity $(\\omega)$, and two Boolean variables $(l,r)$ that encode whether the left and right legs are in contact with the ground. The environment has both discrete and continuous versions, but only the discrete version is used here, in order to include simple models in the comparison. The environment gravity, wind power, and turbulence can also be specified. The starting state is the top center of the space with a random force applied to the lander. The termination state occurs when the lander stops moving or moves outside the frame. The action space is comprised of 4 discrete actions: do nothing, fire left thruster, fire main down thruster, or fire right thruster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Lunar Lander GIF\n",
    "from IPython.display import Image\n",
    "Image(url= \"https://gymnasium.farama.org/_images/lunar_lander.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# !pip install gymnasium\n",
    "# !pip install gymnasium[box2d]\n",
    "# !pip install moviepy\n",
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display environment parameters\n",
    "# References: \n",
    "# https://gymnasium.farama.org/api/env/\n",
    "# https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make(\"LunarLander-v2\", \n",
    "               continuous = False,      # Discrete version\n",
    "               gravity = -10.0, \n",
    "               enable_wind = False, \n",
    "               wind_power = 0.0, \n",
    "               turbulence_power = 0.0, \n",
    "               render_mode=\"rgb_array\") # Render for machine\n",
    "\n",
    "# Reset environment with random number generator seed for reproducibility\n",
    "state, info = env.reset(seed = 21)\n",
    "\n",
    "print(\"Environment metadata:\\n\", env.metadata)\n",
    "print(\"\\nState space:\\n\", env.observation_space)\n",
    "print(\"\\nAction space:\\n\", env.action_space)\n",
    "print(\"\\nStarting state:\\n\", state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below summarizes the descriptive statistics for 100 randomly sampled initial states for this environment. We can see that the initial linear and angular velocities vary more than the position and angular orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Determine average starting state from 100 random samples\n",
    "starting_states = []\n",
    "for _ in range(100):\n",
    "    starting_state, info = env.reset()\n",
    "    starting_states.append(starting_state)\n",
    "\n",
    "features = ['x','y','vx','vy','angle','ang_vel','left_leg','right_leg']\n",
    "starting_states_df = pd.DataFrame(starting_states, columns = features)\n",
    "starting_states_df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models ###\n",
    "Four models are compared in this project: an agentless lunar lander, a random agent, a simple reflex agent, and a deep Q-network agent. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agentless Lunar Lander ####\n",
    "An agentless lunar lander takes no action and is at the complete mercy of its environment. Because the starting state is directly above the landing pad, this might not be as terrible a strategy as it initially seems. The lander will simply fall to the ground in a trajectory determined by the physics of the environmental forces and the initial force placed upon it.\n",
    "\n",
    "#### Random Agent ####\n",
    "\n",
    "A random agent selects any of the possible actions uniformly randomly at each step without regard to the environment. The animation above is an example of such an agent and appears to be a poor strategy. Is this a worse strategy than an agentless lander though?\n",
    "\n",
    "#### Simple Reflex Agent ####\n",
    "\n",
    "A simple reflex agent can only react in a fixed way to sensory input but does not retain any memory of the past and, as a result, cannot learn (Russell & Norvig, 2022). As explained in the environment section, the initial lunar lander position is generally centered above the landing pad, but there is wide variation in initial linear and angular velocity. Thus, the primary task is counteracting any horizontal velocity while at the same time, avoiding turning the ship on its side. Therefore, there is a balance to be achieved, and the programmer must select appropriate threshold constants for the algorithm. Depending on the algorithm and constants, this could potentially be a better strategy than doing nothing or acting randomly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Q-Network Agent ####\n",
    "\n",
    "A deep Q-network (DQN; Mnih et al., 2015, 2016) agent is a deep-learning neural network approximation of a Q-learning agent (Russell & Norvig, 2022; Sutton & Barto, 2018). A Q-learning agent alternates between learning through exploration and exploiting what it has learned by choosing the action with maximal value for each state. Although this is obviously better than the relatively ignorant reflex agent, Q-learning agents can quickly become computationally expensive and slow to learn in large and complex environments (Russell & Norvig, 2022; Sutton & Barto, 2018). \n",
    "\n",
    "Approximate Q-learning agents address this problem by approximating the Q-value of actions in particular states with a weighted feature function based on states and actions (Russell & Norvig, 2022; Sutton & Barto, 2018). However, in the environment in this project, we only have access to the current features and do not know how any particular action will influence those features. \n",
    "\n",
    "Fortunately, the universal approximation theorem tells us that we can approximate any function with a neural network of at least two layers  (Russell & Norvig, 2022). Thus, we can approximate a Q-value function with a DQN by recording a log of previous attempts (i.e., replay buffer) to train the model (Paszke & Towers, 2017). In other words, the agent remembers and learns from its experiences. In addition, this allows us to sample randomly from the buffer to reduce variance (Morales, 2020). Value gradient clipping will be used to \n",
    "\n",
    "For this project, the DQN consists of three fully-connected neural network layers. A non-linear activation function is placed between the layers to prevent the model from collapsing into a one-layer linear model. Here, a rectified linear unit (ReLU) function is selected for this because of its computational simplicity and its consistent, demonstrated effectiveness in previous studies (Goodfellow et al., 2016; Karpathy, 2016). The input to the network are the eight state features and the output is a vector of approximate Q-values for each of the four possible actions given the input state. \n",
    "\n",
    "It is worth noting that image-based convolutional neural networks are frequently used for DQNs (e.g., Mnih et al., 2015). However, I am interested here in solving the lunar landing problem from the perspective of the sensory information available to an agent within the lunar lander, rather than an outside observer.\n",
    "\n",
    "Another design choice is how often the agent should explore versus how often it should exploit what it has learned thus far. For this project, I will use an epsilon greedy policy which prioritizes learning at the beginning but gradually shifts to prioritizing achieving the highest rewards by selecting the policy it has learned thus far.\n",
    "\n",
    "To train the model, I will use the Huber loss function (Huber, 1964), which is more robust to noisy data than mean-squared error for training the model (Paszke & Towers, 2017).For optimization, I will use the AdamW optimization algorithm (Loshchilov & Hutter, 2019). Unlike the stochastic gradient descent, Adam optimization maintains and adapts separate learning rates for each parameter (Brownlee, 2017). AdamW improves upon this by decoupling weight decay from the gradient update (Loshchilov & Hutter, 2019). Gradient clipping will be used to avoid exploding gradients (Goodfellow et al., 2016).\n",
    "\n",
    "A potential problem with training DQNs is that parameter updates cause the targets to shift leading to divergence away from an optimal policy (Morales, 2020). To counteract this two sets of parameters are used: an online set which is updated every iteration and a target set which is updated less frequently, or at a specified rate $\\tau$, and whose Q-values are used to calculate the loss and train the online set. This allows the parameters to stabilize.\n",
    "\n",
    "It is expected that the DQN model will perform better than the first three; however, it is still possible that some difficulties might arise in creating a neural network which converges to an optimal policy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods ###\n",
    "\n",
    "Each model will be run for 1000 episodes and their scores over each of the episodes will be plotted and compared. Visualizing each model's performance over successive episodes permits comparison of how each agent progressed in learning how to solve the model. As Machado and his colleagues (2018) point out, this is preferable to single metrics, such as maximum or average scores which can conceal important distinctions. For example, we can discover whether an agent achieved a maximum only once, but performed poorly most of the time. Similarly, an average score can conceal unstable, fluctuating performance. Ideally, we would like to see an agent learning quickly and then stably maintaining high scores above the solution threshold of 200 points.\n",
    "\n",
    "Applying this methodology, we will be able to explore the advantages and disadvantages of the various models and also demonstrate how a solution to the problem can be explored through the modfication of features and comparison of models.\n",
    "\n",
    "In the following code blocks, I construct the Python classes required to implement each of the agents."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agentless Lunar Lander ####\n",
    "An agentless lunar lander which never fires any of its thrusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define general lunar lander class\n",
    "# Adapted from: \n",
    "# CSPB 3202 Gym tutorial (https://applied.cs.colorado.edu/mod/folder/view.php?id=43136)\n",
    "# https://gymnasium.farama.org/\n",
    "# https://gymnasium.farama.org/api/utils/#save-rendering-videos\n",
    "# https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "# https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py\n",
    "# https://gymnasium.farama.org/tutorials/training_agents/FrozenLake_tuto/\n",
    "\n",
    "from gymnasium.utils.save_video import save_video\n",
    "import numpy as np\n",
    "from IPython.utils.capture import capture_output\n",
    "\n",
    "# Action space\n",
    "NOOP, LEFT, MAIN, RIGHT = 0, 1, 2, 3\n",
    "\n",
    "class LunarLander:\n",
    "    '''\n",
    "    General lunar lander class \n",
    "    subject entirely to its environment\n",
    "    '''   \n",
    "    def __init__(self, name = \"agentless\"):\n",
    "        '''\n",
    "        Initialize agent\n",
    "        '''\n",
    "        self.name = name\n",
    "        self.env = gym.make(\"LunarLander-v2\", \n",
    "               continuous = False,           # Discrete version\n",
    "               gravity = -10.0, \n",
    "               enable_wind = False, \n",
    "               wind_power = 0.0, \n",
    "               turbulence_power = 0.0, \n",
    "               render_mode=\"rgb_array_list\") # Render for machine\n",
    "        \n",
    "        # Reset environment with random number generator seed for reproducibility\n",
    "        self.state, self.info = self.env.reset(seed = 21)\n",
    "        self.features_size = len(self.state)\n",
    "        self.actions_size = env.action_space.n\n",
    "        self.scores = []\n",
    "        self.steps = []\n",
    "        self.policy = []\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        '''\n",
    "        Select action given state\n",
    "        '''\n",
    "        # Do nothing\n",
    "        return NOOP\n",
    "\n",
    "    def episode(self):\n",
    "        '''\n",
    "        Complete episode in given environment\n",
    "        Return score, steps count, and actions log\n",
    "        '''\n",
    "        # Initialize rewards total\n",
    "        score = 0.0\n",
    "\n",
    "        # Initialize steps count\n",
    "        step_count = 0\n",
    "\n",
    "        # Initialize actions list\n",
    "        actions = []\n",
    "\n",
    "        # Start new episode  \n",
    "        state, info = self.env.reset()\n",
    "\n",
    "        # Until end of episode\n",
    "        while True:\n",
    "            # Get action based on current state\n",
    "            action = self.select_action(state)\n",
    "\n",
    "            # Obtain next state, reward, terminated status, truncated status, \n",
    "            # and environment info for given action\n",
    "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            \n",
    "            # Accumulate reward total\n",
    "            score += reward\n",
    "\n",
    "            # Increment steps count\n",
    "            step_count += 1\n",
    "\n",
    "            # Record action\n",
    "            actions.append(action)\n",
    "\n",
    "            # If end of episode\n",
    "            if terminated or truncated:\n",
    "                break  # out of loop\n",
    "            \n",
    "            # Advance to next state \n",
    "            state = next_state\n",
    "\n",
    "        return score, step_count, actions\n",
    "    \n",
    "    def iterate(self, n = 100, verbose = False, video = False):\n",
    "        '''\n",
    "        Attempt task for n episodes\n",
    "        '''\n",
    "        # Initialize logs\n",
    "        scores = []\n",
    "        steps = []\n",
    "        policy = []\n",
    "\n",
    "        video_step_start = 0\n",
    "        \n",
    "        # Repeat n times\n",
    "        for episode in range(n):\n",
    "            # Complete one episode\n",
    "            score, step_count, actions = self.episode()\n",
    "            \n",
    "            # Log scores, steps, policy\n",
    "            scores.append(score)\n",
    "            steps.append(step_count)\n",
    "            policy.append(actions)\n",
    "\n",
    "            # Update object attributes\n",
    "            self.scores = scores\n",
    "            self.steps = steps\n",
    "            self.policy = policy\n",
    "\n",
    "            if verbose:\n",
    "                # Report result\n",
    "                print(f\"Episode {episode} score: {score}\")\n",
    "\n",
    "            if video:\n",
    "                # Suppress MoviePy stdout\n",
    "                # https://stackoverflow.com/a/35624406/14371011\n",
    "                with capture_output() as captured:\n",
    "                    # Save mp4 of every (k^3)th episode (0, 1, 8, 27, ...)\n",
    "                    save_video(self.env.render(),\n",
    "                               video_folder = \"videos\",\n",
    "                               name_prefix = self.name,\n",
    "                               fps = env.metadata[\"render_fps\"],\n",
    "                               step_starting_index = video_step_start,\n",
    "                               episode_index = episode)\n",
    "\n",
    "            # Advance starting step index for videos\n",
    "            video_step_start = step_count + 1           \n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def score_stats(self, verbose = False):\n",
    "        '''\n",
    "        Return mean, max, and min of scores if they exist \n",
    "        '''\n",
    "        # If scores exist\n",
    "        if (len(self.scores) > 0):\n",
    "            # Calculate average, max, and min\n",
    "            average, max, min = np.average(self.scores), np.max(self.scores), np.min(self.scores)\n",
    "            \n",
    "            if verbose:\n",
    "                # Report average, max, and min\n",
    "                print(f\"\\nAverage score: {average}; Max: {max}; Min: {min}\")\n",
    "            return average, max, min\n",
    "        else: \n",
    "            print(\"No scores have been calculated\")\n",
    "            return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reference, the first 41 cubes (from https://oeis.org/A000578) are:\n",
    "cubes = [0, 1, 8, 27, 64, 125, 216, 343, 512, 729, 1000, 1331, 1728, 2197, 2744, 3375, 4096, 4913, 5832, 6859, 8000, 9261, 10648, 12167, 13824, 15625, 17576, 19683, 21952, 24389, 27000, 29791, 32768, 35937, 39304, 42875, 46656, 50653, 54872, 59319, 64000]\n",
    "len(cubes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Agent ####\n",
    "An agent which selects any of the four possible actions randomly at each time step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define random agent class\n",
    "\n",
    "class RandomAgent(LunarLander):\n",
    "    '''\n",
    "    Inherits from lunar lander class\n",
    "    and acts randomly\n",
    "    '''\n",
    "    def __init__(self, name = \"random_agent\"):\n",
    "        LunarLander.__init__(self, name)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        '''\n",
    "        Randomly select action\n",
    "        '''\n",
    "        # Randomly select action from action space\n",
    "        action = self.env.action_space.sample()\n",
    "\n",
    "        return action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Reflex Agent ####\n",
    "\n",
    "This is an agent which can fire its side thrusters in reaction to its current horizontal and angular velocity in relation to threshold constants chosen by the programmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflexAgent(LunarLander):\n",
    "    '''\n",
    "    Inherits from lunar lander class, but \n",
    "    selects actions according to horizontal \n",
    "    and angular position and angular velocity\n",
    "    '''\n",
    "    def __init__(self, name = \"reflex_agent\"):\n",
    "        LunarLander.__init__(self, name)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        '''\n",
    "        Select action according to horizontal and\n",
    "        angular position and angular velocity\n",
    "        '''\n",
    "        # Set tolerances\n",
    "        horiz_vel_tolerance = 0.4\n",
    "        ang_vel_tolerance = 0.001\n",
    "\n",
    "        # Horizontal velocity\n",
    "        vx = state[3]\n",
    "\n",
    "        # Angular velocity\n",
    "        ang_vel = state[5]\n",
    "\n",
    "        # If leftward horizontal velocity\n",
    "        if ((vx < -horiz_vel_tolerance) and\n",
    "            # and clockwise velocity\n",
    "            (ang_vel > ang_vel_tolerance)):\n",
    "\n",
    "            # Then, fire thruster to move right\n",
    "            action = RIGHT\n",
    "        \n",
    "        # If rightward horizontal velocity\n",
    "        elif ((vx > horiz_vel_tolerance) and\n",
    "              # If counter-clockwise velocity\n",
    "              (ang_vel < -ang_vel_tolerance)):\n",
    "            \n",
    "            # Then, fire thruster to move left\n",
    "            action = LEFT\n",
    "\n",
    "        else:\n",
    "            # Do nothing\n",
    "            action = NOOP\n",
    "        \n",
    "        return action "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Q-Network Agent ####\n",
    "\n",
    "A deep Q-network agent has the capacity to remember and learn from its previous experiences using a neural network.\n",
    "\n",
    "##### Replay Memory and Deep Q-Network #####\n",
    "First, we must define a buffer to remember previous experiences and a neural network for the agent to use for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Replay Memory Buffer and Deep Q-Network\n",
    "# Adapted from: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "# Additional references:\n",
    "# https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html#building-models-with-pytorch\n",
    "# https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(21)\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\",device)\n",
    "\n",
    "# Named tuple to encode a single state transition\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# Buffer class to hold and randomly sample recent transitions\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        '''\n",
    "        Initialize buffer with empty double-ended queue\n",
    "        of size \"capacity\"\n",
    "        '''\n",
    "        self.memory = deque([], maxlen = capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        '''\n",
    "        Add a transition to buffer\n",
    "        '''\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''\n",
    "        Randomly sample batch of transitions from buffer\n",
    "        '''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns number of transitions in buffer\n",
    "        '''\n",
    "        return len(self.memory)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Deep Q-Network\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, features_size, actions_size):\n",
    "        '''\n",
    "        Initialize neural network\n",
    "        '''\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # First fully-connected layer: \n",
    "        # Input dimensions: 1 x no. of state feature\n",
    "        # Output dimensions: 1 x 128\n",
    "        self.fc1 = nn.Linear(features_size, 128)\n",
    "\n",
    "        # Second fully-connected layer: \n",
    "        # Input: 1 x 128; Output: 1 x 128\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "\n",
    "        # Final fully-connected layer: \n",
    "        # Input: 1 x 128; \n",
    "        # Output: 1 x no. of possible actions\n",
    "        self.fc3 = nn.Linear(128, actions_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        '''\n",
    "        Forward propagation function\n",
    "        Input: State features (e.g., position, velocity, ...)\n",
    "        Output: tensor of action values\n",
    "        '''\n",
    "        # First layer followed by ReLU activation function\n",
    "        z = F.relu(self.fc1(state))\n",
    "        \n",
    "        # Second layer followed by ReLU activation function\n",
    "        z = F.relu(self.fc2(z))\n",
    "        \n",
    "        # Final output layer\n",
    "        return self.fc3(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128        # Replay buffer batch size\n",
    "gamma = 1.0             # Discount rate\n",
    "eps_max = 0.9           # Epsilon starting value\n",
    "eps_min = 0.05          # Episilon ending value\n",
    "eps_decay = 0.001       # Exponential decay\n",
    "tau = 0.005             # Target network update rate\n",
    "learning_rate = 0.0001  # Optimizer learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_size = env.action_space.n\n",
    "state, info = env.reset(seed = 21)\n",
    "features_size = len(state)\n",
    "\n",
    "# Initialize online and target DQN parameter tensors \n",
    "# and move to selected device\n",
    "online_net = DQN(features_size, actions_size).to(device)\n",
    "target_net = DQN(features_size, actions_size).to(device)\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.AdamW(online_net.parameters(), lr = learning_rate, amsgrad = True)\n",
    "\n",
    "# Initialize Huber loss function\n",
    "criterion = nn.SmoothL1Loss()\n",
    "\n",
    "# Initialize replay memory buffer \n",
    "# with maximum capacity of 10,000 transitions\n",
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon-greedy action selection\n",
    "# Reference: https://pytorch.org/docs/stable/generated/torch.max.html\n",
    "\n",
    "import math\n",
    "\n",
    "# Initialize total training steps count\n",
    "total_steps = 0\n",
    "\n",
    "def select_action(state):\n",
    "    '''\n",
    "    Epsilon-greedy action selection\n",
    "    '''\n",
    "    global total_steps\n",
    "    \n",
    "    # Calculate exponentially decaying epsilon-greedy threshold\n",
    "    # (as number of steps becomes large, threshold becomes minimum)\n",
    "    eps_threshold = (eps_min + (eps_max - eps_min) \n",
    "                         * math.exp(-total_steps * eps_decay))\n",
    "    \n",
    "    # Increment total steps\n",
    "    total_steps += 1\n",
    "\n",
    "    # Epsilon-greedy algorithm\n",
    "    # If random number [0,1) is greater than epsilon threshold\n",
    "    if (random.random() > eps_threshold):\n",
    "        # Without backward propagation \n",
    "        with torch.no_grad():\n",
    "            # Select action with maximum expected reward \n",
    "            # from online network\n",
    "\n",
    "            # Get index of maximum expected reward\n",
    "            max_value_index = online_net(state).max(1)[1]\n",
    "\n",
    "            # Get corresponding action\n",
    "            best_action = max_value_index.view(1,1)\n",
    "\n",
    "            return best_action\n",
    "    else:\n",
    "        # Randomly select action from action space\n",
    "        # and return as tensor\n",
    "        return torch.tensor([[env.action_space.sample()]], \n",
    "                            device = device, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dynamic plotting\n",
    "# References:\n",
    "# https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.isinteractive.html\n",
    "# https://www.statology.org/matplotlib-inline/\n",
    "# https://www.tutorialspoint.com/how-to-check-that-pylab-backend-of-matplotlib-runs-inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display graphs inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Check if inline display is possible\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "\n",
    "# If Jupyter notebook\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "# Enable interactive plotting\n",
    "plt.ion()\n",
    "\n",
    "# Initialize episode scores list\n",
    "scores = []\n",
    "\n",
    "def plot_scores(show_result = False):\n",
    "    '''\n",
    "    Plot episode scores and 100-episode running average \n",
    "    as they become available\n",
    "    '''\n",
    "    # Initialize plot\n",
    "    plt.figure(1)\n",
    "    \n",
    "    # Initialize scores tensor\n",
    "    scores_tensor = torch.tensor(scores, dtype = torch.float)\n",
    "    \n",
    "    # If final result requested\n",
    "    if show_result:\n",
    "        plt.title(\"DQN Agent\")\n",
    "    else:\n",
    "        # Clear current figure\n",
    "        plt.clf()\n",
    "        plt.title(\"Training DQN Agent...\")\n",
    "\n",
    "    # Convert tensor to NumPy array and plot\n",
    "    plt.plot(scores_tensor.numpy(), label = \"Episode Score\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    # If 100 scores are available\n",
    "    if len(scores_tensor) >= 100:\n",
    "        # Calculate 100-episode running average\n",
    "        means = scores_tensor.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        # Plot average\n",
    "        plt.plot(means.numpy(), label = \"100-Episode Average\")\n",
    "        plt.legend()\n",
    "\n",
    "    # Pause to allow plot to update\n",
    "    plt.pause(0.001)\n",
    "\n",
    "    # If Jupyter notebook\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            # Display plot\n",
    "            display.display(plt.gcf())\n",
    "            # Clear plot when done\n",
    "            display.clear_output(wait = True)\n",
    "        else:\n",
    "            # Display plot\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References: \n",
    "# https://stackoverflow.com/questions/19339/transpose-unzip-function-inverse-of-zip/19343\n",
    "# https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "\n",
    "def optimize_model():\n",
    "    '''\n",
    "    Perform single step of optimization on batch of transitions\n",
    "    '''\n",
    "    # If fewer transitions in replay buffer than batch size\n",
    "    if len(memory) < batch_size:\n",
    "        # Can't run batch yet\n",
    "        # so, just keep collecting transitions\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        # Randomly select batch of transitions from\n",
    "        # replay buffer\n",
    "        transitions = memory.sample(batch_size)\n",
    "\n",
    "        # Transpose from batch array of transitions \n",
    "        # to transition of batch-arrays\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Mask to select only non-final states \n",
    "        # (i.e., states whose next state is not null)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: \n",
    "                                                s is not None, \n",
    "                                                batch.next_state)), \n",
    "                                                device = device, \n",
    "                                                dtype = torch.bool)\n",
    "\n",
    "        # Concatenate the non-null next states\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                            if s is not None])\n",
    "        \n",
    "        # Concatenate states, actions, and rewards\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Calculate online network Q-values for each state and action, Q(s,a) \n",
    "        online_Q_values = online_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Initialize tensor of next state expected rewards, V(s') = 0\n",
    "        target_next_state_values = torch.zeros(batch_size, device = device)\n",
    "                \n",
    "        # Without back propagation\n",
    "        with torch.no_grad():\n",
    "            # Calculate target network expected reward (i.e., value) \n",
    "            # for each next state, V(s')\n",
    "            target_next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "        \n",
    "        # Calculate target network expected Q values \n",
    "        target_expected_Q_values = ((target_next_state_values * gamma) \n",
    "                                        + reward_batch)\n",
    "\n",
    "        # # Initialize Huber loss function\n",
    "        # criterion = nn.SmoothL1Loss()\n",
    "\n",
    "        # Calculate loss by comparing online and target network Q values\n",
    "        loss = criterion(online_Q_values, \n",
    "                         target_expected_Q_values.unsqueeze(1))\n",
    "\n",
    "        # Reset optimizer gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Back propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform gradient clipping by value to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_value_(online_net.parameters(), 100)\n",
    "        \n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN model training\n",
    "\n",
    "# If GPU device is available\n",
    "if torch.cuda.is_available():\n",
    "    episodes = 1000\n",
    "else:\n",
    "    episodes = 100\n",
    "\n",
    "# Repeat for specified number of episodes\n",
    "for episode in range(episodes):\n",
    "\n",
    "    # Start new episode and get state\n",
    "    state, info = env.reset()\n",
    "\n",
    "    #Convert to tensor\n",
    "    state = torch.tensor(state, \n",
    "                         dtype = torch.float32, \n",
    "                         device = device).unsqueeze(0)\n",
    "    \n",
    "    # Initialize episode score\n",
    "    score = 0.0\n",
    "\n",
    "    # Until end of episode\n",
    "    while True:\n",
    "        # Get action based on current state\n",
    "        action = select_action(state)\n",
    "\n",
    "        # Obtain next state, reward, terminated status, truncated status, \n",
    "        # and environment info for given action    \n",
    "        next_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "        \n",
    "        # Convert reward to tensor\n",
    "        reward = torch.tensor([reward], device = device)\n",
    "\n",
    "        # Accumulate episode reward total\n",
    "        score += reward\n",
    "        \n",
    "        # If action taken leads to terminated state\n",
    "        if terminated:\n",
    "            # Next state is null\n",
    "            next_state = None\n",
    "        else:\n",
    "            # Convert next state to tensor\n",
    "            next_state = torch.tensor(next_state, \n",
    "                                      dtype = torch.float32, \n",
    "                                      device = device).unsqueeze(0)\n",
    "\n",
    "        # Store transition in replay buffer\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of optimization on online network\n",
    "        optimize_model()\n",
    "\n",
    "        # Get target and online network state dicts\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        online_net_state_dict = online_net.state_dict()\n",
    "\n",
    "        # For each key in the online network state dict\n",
    "        for key in online_net_state_dict:\n",
    "            # Update target network weights at rate (1-tau)\n",
    "            # theta' <-- tau(theta) + (1-tau)theta'\n",
    "            target_net_state_dict[key] = (online_net_state_dict[key]*tau \n",
    "                                          + target_net_state_dict[key]*(1-tau))\n",
    "        \n",
    "        # Load updated target network state dict\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        # If end of episode\n",
    "        if terminated or truncated:\n",
    "            # Log score\n",
    "            scores.append(score)\n",
    "            # Update plot\n",
    "            plot_scores()\n",
    "            break\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot final result\n",
    "plot_scores(show_result = True)\n",
    "plt.ioff()  # Disable interactive plotting\n",
    "plt.show(); # Display plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DQN Agent ####\n",
    "Now we can define the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(LunarLander):\n",
    "    '''\n",
    "    Inherits from lunar lander class, but \n",
    "    iteratively learns from experience to approximate \n",
    "    Q-values from state features with a neural network\n",
    "    '''\n",
    "    def __init__(self, name = \"dqn_agent\", alpha = 0.5, epsilon = 0.1):\n",
    "        LunarLander.__init__(self, name)\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def update(self, state, action, reward, new_state):\n",
    "        '''\n",
    "        Updates weights\n",
    "        '''\n",
    "        return None\n",
    "\n",
    "    def select_action(self, state):\n",
    "        '''\n",
    "        Select action\n",
    "        '''\n",
    "                \n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results ## \n",
    "\n",
    "In this section, I compare the five different models: an agentless lunar lander, a random agent, a simple reflex agent, and a DQN agent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agentless Lunar Lander ###\n",
    "First, we consider the agentless lunar lander which never fires any of its thrusters and is therefore subject entirely to its environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate agentless lunar lander\n",
    "non_agent = LunarLander()\n",
    "\n",
    "# Number of episodes to iterate over\n",
    "episodes = 100\n",
    "\n",
    "# Iterate over 100 episodes\n",
    "non_scores = non_agent.iterate(episodes, verbose = False, video = True)\n",
    "\n",
    "# Print average, max, min\n",
    "average, max, min = non_agent.score_stats(verbose = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to the video of the agentless lander in the 65<sup>th</sup> episode](videos/agentless-episode-64.mp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot agentless lunar lander scores over all episodes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(non_scores, label = \"Agentless Lander\")\n",
    "plt.axhline(y = 200, color = \"r\", linestyle = \"dashed\", label = \"Solution Threshold\")  \n",
    "plt.xlim([0,episodes])\n",
    "plt.ylim([-600,300])\n",
    "plt.title(\"Agentless Lunar Lander\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above graph, we can see that an agentless lunar lander is not sufficient to solve this problem. In the video linked above, we can see the lunar lander simply falling in a slightly curved trajectory toward the ground."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent ###\n",
    "\n",
    "Next, we consider the random agent which selects an action randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate lunar lander random agent\n",
    "rand_agent = RandomAgent()\n",
    "\n",
    "# Iterate over episodes\n",
    "rand_scores = rand_agent.iterate(episodes, verbose = False, video = True)\n",
    "\n",
    "# Print average, max, min\n",
    "average, max, min = rand_agent.score_stats(verbose = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to the video of the random agent in the 65<sup>th</sup> episode](videos/random_agent-episode-64.mp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot agentless lander and random agent scores over all episodes\n",
    "\n",
    "plt.plot(non_scores, label = \"Agentless Lander\")\n",
    "plt.plot(rand_scores, label = \"Random Agent\")\n",
    "plt.axhline(y = 200, color = \"r\", linestyle = \"dashed\", label = \"Solution Threshold\")  \n",
    "plt.xlim([0,episodes])\n",
    "plt.ylim([-600,300])\n",
    "plt.title(\"Random Agent vs. Agentless Lunar Lander\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above graph shows, doing nothing is better than acting randomly in this environment. The agentless lunar lander is relatively more consistent and successful compared to the random agent and also has a slightly higher average score. Obviously, neither learn anything nor come remotely close to consistently reaching the solution threshold, as expected."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Reflex Agent ###\n",
    "\n",
    "Next, we examine the agent which can reflexively fire its side engines in reaction to its current horizontal and angular velocity mediated by pre-selected threshold constants. Choosing ideal threshold constants requires tedious trial and error on the part of the programmer, rather than the agent. Thus, only the programmer is learning in this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate lunar lander reflex agent\n",
    "rflx_agent = ReflexAgent()\n",
    "\n",
    "# Iterate over episodes\n",
    "rflx_scores = rflx_agent.iterate(episodes, verbose = False, video = True)\n",
    "\n",
    "# Print average max, min\n",
    "average, max, min = rflx_agent.score_stats(verbose = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to the video of the reflex agent in the 65<sup>th</sup> episode](videos/reflex_agent-episode-64.mp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot agentless, random, and reflex lander scores over all episodes\n",
    "\n",
    "plt.plot(non_scores, label = \"Agentless Lander\")\n",
    "plt.plot(rand_scores, label = \"Random Agent\")\n",
    "plt.plot(rflx_scores, label = \"Reflex Agent\")\n",
    "plt.axhline(y = 200, color = \"r\", linestyle = \"dashed\", label = \"Solution Threshold\")  \n",
    "plt.xlim([0,episodes])\n",
    "plt.ylim([-600,300])\n",
    "plt.title(\"Random Agent, Reflex Agent, and Agentless Lander\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above graph, we can see that the reflex agent is only slightly more consistent and successful than the agentless lunar lander but still not even close to solving the problem. Additionally, it is clearly not learning, as expected. Landing a lunar rover appears to be more difficult than firing the left and right thrusters based on horizontal and angular velocity. It might be possible to achieve a higher score by comparing various algorithms and parameters through trial and error, but this would be a tedious process, so we would like to find a more expedient way to find the best policy to follow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Network Agent ###\n",
    "\n",
    "Next, we examine the performance of a remembering, learning deep Q-network agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate lunar lander DQN agent\n",
    "# dqn_agent = DQNAgent()\n",
    "\n",
    "# # Iterate over episodes\n",
    "# dqn_scores = dqn_agent.iterate(episodes, verbose = False, video = True)\n",
    "\n",
    "# # Print average, max, min\n",
    "# average, max, min = dqn_agent.score_stats(verbose = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to the video of the DQN agent in the 65<sup>th</sup> episode](videos/dqn_agent-episode-64.mp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot agentless, random, reflex, and DQN lander scores over all episodes\n",
    "\n",
    "# plt.plot(non_scores, label = \"Agentless Lander\")\n",
    "# plt.plot(rand_scores, label = \"Random Agent\")\n",
    "# plt.plot(rflx_scores, label = \"Reflex Agent\")\n",
    "# plt.plot(dqn_scores, label = \"DQN Agent\")\n",
    "# plt.axhline(y = 200, color = \"r\", linestyle = \"dashed\", label = \"Solution Threshold\")  \n",
    "# plt.xlim([0,episodes])\n",
    "# plt.ylim([-600,300])\n",
    "# plt.title(\"All-Model Comparison\")\n",
    "# plt.xlabel(\"Episode\")\n",
    "# plt.ylabel(\"Score\")\n",
    "# plt.legend()\n",
    "# plt.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion ## \n",
    "\n",
    "Does it include discussion (what went well or not and why), and suggestions for improvements or future work?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggestions for Future Research ##\n",
    "\n",
    "Stochasticity: frame skipping, sticky actions that repeat with a given probability (Macado et al., 2018)\n",
    "\n",
    "Transfer learning\n",
    "\n",
    "Convolutional network\n",
    "\n",
    "Arcade Learning Environment (Bellemare, 2023; Bellemare et al., 2013; Machado et al., 2018) simulating classic Atari 2600 games (AtariAge, 2023)\n",
    "\n",
    "_AtariAge._ (2023). [Website]. [https://atariage.com](https://atariage.com)\n",
    "\n",
    "Bellemare, M. G. (2023). Arcade Learning Environment. [https://github.com/mgbellemare/Arcade-Learning-Environment](https://github.com/mgbellemare/Arcade-Learning-Environment)\n",
    "\n",
    "Bellemare, M. G., Naddaf, Y., Veness, J., & Bowling, M. (2013). The Arcade Learning Environment: An evaluation platform for general agents. _Journal of Artificial Intelligence Research, 47,_ 253-279.\n",
    "[https://doi.org/10.1613/jair.3912](https://doi.org/10.1613/jair.3912)\n",
    " \n",
    "Finally, it is only right to ask: What is the value of crashing a virtual lunar lander model thousands of times in order to eventually land successfully? The answer is it is better to train a far less-expensive virtual model and then transfer that learning to a real-world environment. Thus, the long-term aim is to develop the ability to transfer this virtual knowledge to a real-world environment.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References ##\n",
    "\n",
    "Brownlee, J. (2017, July 3). Gentle introduction to the Adam optimization algorithm for deep learning. Machine Learning Mastery. [https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
    "\n",
    "Goodfellow, I., Bengio, Y., & Courville, A. (2016). _Deep learning._ MIT Press. [http://www.deeplearningbook.org](http://www.deeplearningbook.org)\n",
    "\n",
    "_Gymnasium._ (2022). Farama Foundation. [https://gymnasium.farama.org/](https://gymnasium.farama.org/)\n",
    "\n",
    "Heintz, B. (2021, April 17). _Building models with Pytorch._ Pytorch. [https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html#building-models-with-pytorch](https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html#building-models-with-pytorch)\n",
    "\n",
    "Huber, P. J. (1964). Robust estimation of a location parameter. _Annals of Statistics, 53_(1), 73–101. [https//doi.org/10.1214/aoms/1177703732](https//doi.org/10.1214/aoms/1177703732)\n",
    "\n",
    "Karpathy, A. (2016). _Convolutional neural networks for visual recognition._ Stanford University. [https://cs231n.github.io/convolutional-networks/](https://cs231n.github.io/convolutional-networks/)\n",
    "\n",
    "Loshchilov, I., & Hutter, F. (2019). Decoupled weight decay regularization. _International Conference on Learning Representations._[https://doi.org/10.48550/arXiv.1711.05101](https://doi.org/10.48550/arXiv.1711.05101)\n",
    "\n",
    "Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., & Bowling, M. (2018). Revisiting the Arcade Learning Environment: Evaluation protocols and open problems for general agents. _Journal of Artificial Intelligence Research, 61,_ 523–562. [https://doi.org/10.1613/jair.5699](https://doi.org/10.1613/jair.5699)\n",
    "\n",
    "Matplotlib. (2023). _API reference._ [https://matplotlib.org/stable/api/index](https://matplotlib.org/stable/api/index)\n",
    "\n",
    "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., & Hassabis, D. (2015). Human-level control through deep reinforcement learning. _Nature, 518_, 529–533. [https://doi.org/10.1038/nature14236](https://doi.org/10.1038/nature14236)\n",
    "\n",
    "Mnih, V., Puigdomènech Badia, A., Mirza, M., Graves, A., Harley, T., Lillicrap, T. P., Silver, D., & Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. _Proceedings of the 33rd International Conference on Machine Learning, 48,_ 1928–1937. [https://proceedings.mlr.press/v48/mniha16.html](https://proceedings.mlr.press/v48/mniha16.html)\n",
    "\n",
    "Morales, M. (2020). _Grokking deep reinforcement learning._ Simon and Schuster.\n",
    "\n",
    "Paszke, A., & Towers, M. (2017, April 6). _Reinforcement learning (DQN) tutorial._ Pytorch. [https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    "\n",
    "Pytorch. (2023). _Pytorch documentation._ [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)\n",
    "\n",
    "Russell, S., & Norvig, R. (2022). Artificial intelligence: A modern approach, (4th ed.). Pearson. \n",
    "\n",
    "Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT Press."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
