{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xlvhbtnstVV_"
      },
      "source": [
        "# CSPB 3202 Final Project #\n",
        "\n",
        "Tyler Kinkade, Tyler.Kinkade@colorado.edu    \n",
        "\n",
        "_All rights reserved_\n",
        "\n",
        "GitHub: [https://github.com/jaki9292/rl-project](https://github.com/jaki9292/rl-project)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aDi3EokJtVWB"
      },
      "source": [
        "## Overview ##\n",
        "\n",
        "This write-up reports on a small project to compare the effectiveness of artificial agent algorithms ranging in sophistication from naive to deep reinforcement learning (Russell & Norvig, 2022; Sutton & Barto, 2018) to successfully land a virtual \"lunar lander\" in a Gymnasium (2022) model environment.\n",
        "\n",
        "The report is divided into three main sections: approach, results, and discussion."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oxfx3dpPtVWB"
      },
      "source": [
        "## Approach ##\n",
        "\n",
        "My approach to the problem is described in the following environment, models, and methods subsections."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KdB4w2WltVWC"
      },
      "source": [
        "### Environment ###\n",
        "\n",
        "The lunar lander environment (pictured below) simulates rocket trajectory physics with the primary aim of landing a lunar lander on a central landing pad (marked by two flags) by means of turning its three rockets thrusters (left, right, and main) on or off. The agent operating the lander is rewarded for landing slowly, in an upright position, on both legs, on or near the landing pad and penalized otherwise. Fuel is unlimited, but a penalty is given for each time an engine fires. Episodes are truncated after 1000 time-steps. Scores of 200 points or more are considered a solution. \n",
        "\n",
        "The state space is described by an 8-vector comprised of the lander coordinates $(x,y)$, its linear velocities $(v_x,v_y)$ in the $x$ and $y$ directions, its angle $(\\theta)$, its angular velocity $(\\omega)$, and two Boolean variables $(l,r)$ that encode whether the left and right legs are in contact with the ground. The environment has both discrete and continuous versions, but only the discrete version is used here, in order to include simple models in the comparison. The environment gravity, wind power, and turbulence can also be specified. The starting state is the top center of the space with a random force applied to the lander. The termination state occurs when the lander stops moving or moves outside the frame. The action space is comprised of 4 discrete actions: do nothing, fire left thruster, fire main down thruster, or fire right thruster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "DfDyQ_Q6tVWC",
        "outputId": "364740f7-3360-4ad3-a556-ff95baae1f48"
      },
      "outputs": [],
      "source": [
        "# Display Lunar Lander GIF\n",
        "from IPython.display import Image\n",
        "Image(url= \"https://gymnasium.farama.org/_images/lunar_lander.gif\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACpbtMfztVWE"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "# Packages preinstalled on Google Colab:\n",
        "# !pip install numpy\n",
        "# !pip install pandas\n",
        "# !pip install matplotlib\n",
        "# !pip install torch\n",
        "\n",
        "# Packages not pre-installed on Google Colab:\n",
        "# !pip install gymnasium\n",
        "# !pip install gymnasium[box2d]\n",
        "# !pip install moviepy\n",
        "# !pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGBVDYBDtVWF",
        "outputId": "da48a956-65d0-451a-b780-6c8e2d5afd0f"
      },
      "outputs": [],
      "source": [
        "# Display environment parameters\n",
        "# References: \n",
        "# https://gymnasium.farama.org/api/env/\n",
        "# https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
        "\n",
        "import gymnasium as gym\n",
        "\n",
        "# Initialize environment\n",
        "env = gym.make(\"LunarLander-v2\", \n",
        "               continuous = False,      # Discrete version\n",
        "               gravity = -10.0, \n",
        "               enable_wind = False, \n",
        "               wind_power = 0.0, \n",
        "               turbulence_power = 0.0, \n",
        "               render_mode=\"rgb_array\") # Render for machine\n",
        "\n",
        "# Reset environment with random number generator seed for reproducibility\n",
        "state, info = env.reset(seed = 21)\n",
        "\n",
        "print(\"Environment metadata:\\n\", env.metadata)\n",
        "print(\"\\nState space:\\n\", env.observation_space)\n",
        "print(\"\\nAction space:\\n\", env.action_space)\n",
        "print(\"\\nStarting state:\\n\", state)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j9WYmiQbtVWF"
      },
      "source": [
        "The table below summarizes the descriptive statistics for 100 randomly sampled initial states for this environment. We can see that the initial linear and angular velocities vary more than the position and angular orientation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "xkBzvsWztVWF",
        "outputId": "f3057c8d-ab76-4a53-935e-f0cfe981a880"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Determine average starting state from 100 random samples\n",
        "starting_states = []\n",
        "for _ in range(100):\n",
        "    starting_state, info = env.reset()\n",
        "    starting_states.append(starting_state)\n",
        "\n",
        "features = ['x','y','vx','vy','angle','ang_vel','left_leg','right_leg']\n",
        "starting_states_df = pd.DataFrame(starting_states, columns = features)\n",
        "starting_states_df.describe()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "86xigl67tVWG"
      },
      "source": [
        "### Models ###\n",
        "Four models are compared in this project: an agentless lunar lander, a random agent, a simple reflex agent, and a deep Q-network agent. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "s3etHtRptVWG"
      },
      "source": [
        "#### Agentless Lunar Lander ####\n",
        "An agentless lunar lander takes no action and is at the complete mercy of its environment. Because the starting state is directly above the landing pad, this might not be as terrible a strategy as it initially seems. The lander will simply fall to the ground in a trajectory determined by the physics of the environmental forces and the initial force placed upon it.\n",
        "\n",
        "#### Random Agent ####\n",
        "\n",
        "A random agent selects any of the possible actions uniformly randomly at each step without regard to the environment. The animation above is an example of such an agent and appears to be a poor strategy. Is this a worse strategy than an agentless lander though?\n",
        "\n",
        "#### Simple Reflex Agent ####\n",
        "\n",
        "A simple reflex agent can only react in a fixed way to sensory input but does not retain any memory of the past and, as a result, cannot learn (Russell & Norvig, 2022). As explained in the environment section, the initial lunar lander position is generally centered above the landing pad, but there is wide variation in initial linear and angular velocity. Thus, the primary task is counteracting any horizontal velocity while at the same time, avoiding turning the ship on its side. Therefore, there is a balance to be achieved, and the programmer must select appropriate threshold constants for the algorithm. Depending on the algorithm and constants, this could potentially be a better strategy than doing nothing or acting randomly."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IQR8bnZqtVWG"
      },
      "source": [
        "#### Deep Q-Network Agent ####\n",
        "\n",
        "A deep Q-network (DQN; Mnih et al., 2015, 2016) agent is a deep-learning neural network approximation of a Q-learning agent (Russell & Norvig, 2022; Sutton & Barto, 2018). A Q-learning agent alternates between learning through exploration and exploiting what it has learned by choosing the action with maximal value for each state. Although this is obviously better than the relatively ignorant reflex agent, Q-learning agents can quickly become computationally expensive and slow to learn in large and complex environments (Russell & Norvig, 2022; Sutton & Barto, 2018). \n",
        "\n",
        "Approximate Q-learning agents address this problem by approximating the Q-value of actions in particular states with a weighted feature function based on states and actions (Russell & Norvig, 2022; Sutton & Barto, 2018). However, in the environment in this project, we only have access to the current features and do not know how any particular action will influence those features. \n",
        "\n",
        "Fortunately, the universal approximation theorem tells us that we can approximate any function with a neural network of at least two layers  (Russell & Norvig, 2022). Thus, we can approximate a Q-value function with a DQN by recording a log of previous attempts (i.e., replay buffer) to train the model (Paszke & Towers, 2017). In other words, the agent remembers and learns from its experiences. In addition, this allows us to sample randomly from the buffer to reduce variance (Morales, 2020). Value gradient clipping will be used to \n",
        "\n",
        "For this project, the DQN consists of three fully-connected neural network layers. A non-linear activation function is placed between the layers to prevent the model from collapsing into a one-layer linear model. Here, a rectified linear unit (ReLU) function is selected for this because of its computational simplicity and its consistent, demonstrated effectiveness in previous studies (Goodfellow et al., 2016; Karpathy, 2016). The input to the network are the eight state features and the output is a vector of approximate Q-values for each of the four possible actions given the input state. \n",
        "\n",
        "It is worth noting that image-based convolutional neural networks are frequently used for DQNs (e.g., Mnih et al., 2015). However, I am interested here in solving the lunar landing problem from the perspective of the sensory information available to an agent within the lunar lander, rather than an outside observer.\n",
        "\n",
        "Another design choice is how often the agent should explore versus how often it should exploit what it has learned thus far. For this project, I will use an epsilon greedy policy which prioritizes learning at the beginning but gradually shifts to prioritizing achieving the highest rewards by selecting the policy it has learned thus far.\n",
        "\n",
        "To train the model, I will use the Huber loss function (Huber, 1964), which is more robust to noisy data than mean-squared error for training the model (Paszke & Towers, 2017).For optimization, I will use the AdamW optimization algorithm (Loshchilov & Hutter, 2019). Unlike the stochastic gradient descent, Adam optimization maintains and adapts separate learning rates for each parameter (Brownlee, 2017). AdamW improves upon this by decoupling weight decay from the gradient update (Loshchilov & Hutter, 2019). Gradient clipping will be used to avoid exploding gradients (Goodfellow et al., 2016).\n",
        "\n",
        "A potential problem with training DQNs is that parameter updates cause the targets to shift leading to divergence away from an optimal policy (Morales, 2020). To counteract this two sets of parameters are used: an online set which is updated every iteration and a target set which is updated less frequently, or at a specified rate $\\tau$, and whose Q-values are used to calculate the loss and train the online set. This allows the parameters to stabilize.\n",
        "\n",
        "It is expected that the DQN model will perform better than the first three; however, it is still possible that some difficulties might arise in creating a neural network which converges to an optimal policy."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ah58GhAitVWH"
      },
      "source": [
        "### Methods ###\n",
        "\n",
        "The first three models do not require training, so they will be run for 100 episodes because their average score will not differ significantly after even more episodes. The DQN model will likely require training for at least 500 episodes as well as hyperparameter tuning. For all models, the single-episode truncation will be reduced to 600 time-steps in order to decrease the training time.\n",
        "\n",
        "The scores over each of the episodes will be plotted and compared. Visualizing each model's performance over successive episodes permits comparison of how each agent progressed in learning how to solve the model. As Machado and his colleagues (2018) point out, this is preferable to single metrics, such as maximum or average scores which can conceal important distinctions. For example, we can discover whether an agent achieved a maximum only once, but performed poorly most of the time. Similarly, an average score can conceal unstable, fluctuating performance. Ideally, we would like to see an agent learning quickly and then stably maintaining high scores above the solution threshold of 200 points.\n",
        "\n",
        "Applying this methodology, we will be able to explore the advantages and disadvantages of the various models and also demonstrate how a solution to the problem can be explored through the modfication of features and comparison of models.\n",
        "\n",
        "In the following code blocks, I construct the Python classes required to implement each of the agents."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NWD40CbatVWH"
      },
      "source": [
        "#### Agentless Lunar Lander ####\n",
        "An agentless lunar lander which never fires any of its thrusters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsUcVw-RtVWH"
      },
      "outputs": [],
      "source": [
        "# Define general lunar lander class\n",
        "#\n",
        "# Adapted from: \n",
        "# CSPB 3202 Gym tutorial (https://applied.cs.colorado.edu/mod/folder/view.php?id=43136)\n",
        "# https://gymnasium.farama.org/\n",
        "# https://gymnasium.farama.org/api/utils/#save-rendering-videos\n",
        "# https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
        "# https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py\n",
        "# https://gymnasium.farama.org/tutorials/training_agents/FrozenLake_tuto/\n",
        "# https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "#\n",
        "# Additional references:\n",
        "# https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.isinteractive.html\n",
        "# https://www.statology.org/matplotlib-inline/\n",
        "# https://www.tutorialspoint.com/how-to-check-that-pylab-backend-of-matplotlib-runs-inline\n",
        "# https://learnpython.com/blog/average-in-matplotlib/\n",
        "\n",
        "from gymnasium.utils.save_video import save_video\n",
        "import numpy as np\n",
        "from IPython.utils.capture import capture_output\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Display graphs inline\n",
        "%matplotlib inline\n",
        "\n",
        "# Check if inline display is possible\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "\n",
        "# If Jupyter notebook\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "class LunarLander:\n",
        "    '''\n",
        "    General lunar lander class \n",
        "    subject entirely to its environment\n",
        "    '''   \n",
        "    def __init__(self, name = \"Agentless Lander\", step_limit = 600):\n",
        "        '''\n",
        "        Initialize agent\n",
        "        '''\n",
        "        self.name = name\n",
        "        self.prefix = \"-\".join(name.split()).lower() # filename prefix\n",
        "        self.step_limit = step_limit  # max. allowed steps/episode\n",
        "\n",
        "        # Initialize environment\n",
        "        self.env = gym.make(\"LunarLander-v2\", \n",
        "               continuous = False,           # Discrete version\n",
        "               gravity = -10.0, \n",
        "               enable_wind = False, \n",
        "               wind_power = 0.0, \n",
        "               turbulence_power = 0.0, \n",
        "               render_mode=\"rgb_array_list\") # Render for machine\n",
        "\n",
        "        # Define actions\n",
        "        self.NOOP = 0   # No operation (i.e., do nothing)\n",
        "        self.LEFT = 1   # Fire left orientation engine\n",
        "        self.MAIN = 2   # Fire main downward engine\n",
        "        self.RIGHT = 3  # Fire right orientation engine\n",
        "\n",
        "        # Reset environment with random number generator seed for reproducibility\n",
        "        self.state, self.info = self.env.reset(seed = 21)\n",
        "        \n",
        "        # Set environment dimensions\n",
        "        self.features_size = len(self.state)\n",
        "        self.actions_size = env.action_space.n\n",
        "  \n",
        "        # Initialize data logs\n",
        "        self.scores = []\n",
        "        self.steps = []\n",
        "        self.policy = []\n",
        "\n",
        "        # Initialize total iteration step count\n",
        "        self.total_steps = 0\n",
        "\n",
        "        # Set learning rate scheduler to null\n",
        "        self.scheduler = None\n",
        "\n",
        "    def select_action(self, state):\n",
        "        '''\n",
        "        Select action given state\n",
        "        '''\n",
        "        # Do nothing\n",
        "        return self.NOOP\n",
        "\n",
        "    def episode(self):\n",
        "        '''\n",
        "        Complete episode in given environment\n",
        "        Return score, steps count, and actions log\n",
        "        '''\n",
        "        # Initialize rewards total\n",
        "        score = 0.0\n",
        "\n",
        "        # Initialize steps count\n",
        "        episode_steps = 0\n",
        "\n",
        "        # Initialize actions list\n",
        "        actions = []\n",
        "\n",
        "        # Start new episode  \n",
        "        state, info = self.env.reset()\n",
        "\n",
        "        # Until end of episode\n",
        "        while True:\n",
        "            # Get action based on current state\n",
        "            action = self.select_action(state)\n",
        "\n",
        "            # Obtain next state, reward, terminated status, truncated status, \n",
        "            # and environment info for given action\n",
        "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
        "            \n",
        "            # Accumulate reward total\n",
        "            score += reward\n",
        "\n",
        "            # Increment episode steps\n",
        "            episode_steps += 1\n",
        "\n",
        "            # Increment overall step count\n",
        "            self.total_steps += 1\n",
        "\n",
        "            # Record action\n",
        "            actions.append(action)\n",
        "\n",
        "            # If number of steps has been surpassed or end of episode\n",
        "            if (episode_steps > self.step_limit) or terminated or truncated:\n",
        "                break  # out of loop\n",
        "            \n",
        "            # Advance to next state \n",
        "            state = next_state\n",
        "\n",
        "        return score, episode_steps, actions\n",
        "    \n",
        "    def iterate(self, n = 100,      # Number of iterations\n",
        "                verbose = False,    # Print results for each episode\n",
        "                video = False,      # Output videos of (k^3)th episodes\n",
        "                plot = True):       # Display plot updated every episode\n",
        "        '''\n",
        "        Attempt task for n episodes\n",
        "        '''\n",
        "        # Enable interactive plotting\n",
        "        plt.ion()\n",
        "\n",
        "        # Counter for indexing start of videos\n",
        "        video_step_start = 0\n",
        "\n",
        "        # Repeat n times\n",
        "        for episode in range(n):\n",
        "            # Complete one episode\n",
        "            score, episode_steps, actions = self.episode()\n",
        "            \n",
        "            # Log scores, steps, policy\n",
        "            self.scores.append(score)\n",
        "            self.steps.append(episode_steps)\n",
        "            self.policy.append(actions)\n",
        "\n",
        "            # If learning rate scheduler is specified\n",
        "            if self.scheduler:                        \n",
        "                # Increment learning rate scheduler count\n",
        "                # Scheduler will perform learning rate decrement\n",
        "                # after kth episode specified in lr_period parameter \n",
        "                self.scheduler.step()\n",
        "            \n",
        "            if plot:\n",
        "                # Update training plot\n",
        "                self.plot_scores()\n",
        "\n",
        "            if verbose:\n",
        "                # Report result\n",
        "                print(f\"Episode {episode} score: {score}\")\n",
        "\n",
        "            if video:\n",
        "                # Suppress MoviePy stdout\n",
        "                # https://stackoverflow.com/a/35624406/14371011\n",
        "                with capture_output() as captured:\n",
        "                    # Save mp4 of every (k^3)th episode (0, 1, 8, 27, ...)\n",
        "                    save_video(self.env.render(),\n",
        "                               video_folder = \"videos\",\n",
        "                               name_prefix = self.prefix,\n",
        "                               fps = env.metadata[\"render_fps\"],\n",
        "                               step_starting_index = video_step_start,\n",
        "                               episode_index = episode)\n",
        "\n",
        "            # Advance starting step index for videos\n",
        "            video_step_start = episode_steps + 1           \n",
        "        \n",
        "        if plot:\n",
        "            # Plot final result\n",
        "            self.plot_scores(show_result = True)\n",
        "            plt.show() # Display plot\n",
        "\n",
        "        # Disable interactive plotting\n",
        "        plt.ioff() \n",
        "    \n",
        "    def score_stats(self, verbose = False):\n",
        "        '''\n",
        "        Return mean, max, and min of scores if they exist \n",
        "        '''\n",
        "        # If scores exist\n",
        "        if (len(self.scores) > 0):\n",
        "            # Calculate average, max, and min score\n",
        "            average = np.average(self.scores)\n",
        "            max = np.max(self.scores)\n",
        "            min = np.min(self.scores)\n",
        "            \n",
        "            # Calculate average episode length\n",
        "            avg_length = np.average(self.steps)\n",
        "\n",
        "            if verbose:\n",
        "                # Report average, max, and min\n",
        "                print(f\"\\nAverage score: {average}\") \n",
        "                print(f\"Maximum score: {max}\")\n",
        "                print(f\"Minimum score: {min}\")\n",
        "                print(f\"Average episode length: {avg_length}\")\n",
        "\n",
        "            return average, max, min, avg_length\n",
        "        else: \n",
        "            print(\"No scores have been calculated\")\n",
        "            return None, None, None\n",
        "        \n",
        "    def plot_scores(self, show_result = False):\n",
        "        '''\n",
        "        Dynamically plot episode scores\n",
        "        Plot running average at conclusion \n",
        "        '''\n",
        "        # Initialize plot\n",
        "        plt.figure(1)\n",
        "\n",
        "        # If not final plot\n",
        "        if not show_result:\n",
        "            # Clear current figure\n",
        "            plt.clf()\n",
        "\n",
        "        # Plot solution threshold\n",
        "        plt.axhline(y = 200, color = \"r\", linestyle = \"dashed\", label = \"Solution Threshold\")  \n",
        "\n",
        "        # Plot episode-by-episode scores\n",
        "        plt.plot(self.scores, label = self.name)\n",
        "        plt.title(self.name)\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Score\")\n",
        "\n",
        "        # If final plot\n",
        "        if show_result:\n",
        "            # Plot running average\n",
        "\n",
        "            # Span for running average\n",
        "            window = 25\n",
        "            \n",
        "            # Initialize list with one fewer NaNs than window\n",
        "            averages = [np.nan] * (window - 1)\n",
        "\n",
        "            # Calculate running average\n",
        "            for idx in range(len(self.scores) - window + 1):\n",
        "                averages.append(np.mean(self.scores[idx : idx + window]))\n",
        "\n",
        "            # Plot running average\n",
        "            plt.plot(averages, color = \"k\", label = \"Running Average\")\n",
        "\n",
        "        plt.legend()\n",
        "\n",
        "        # Pause to allow plot to update\n",
        "        plt.pause(0.001)\n",
        "\n",
        "        # If Jupyter notebook\n",
        "        if is_ipython:\n",
        "            if not show_result:\n",
        "                # Display plot\n",
        "                display.display(plt.gcf())\n",
        "                # Clear plot when done\n",
        "                display.clear_output(wait = True)\n",
        "            else:\n",
        "                # Display plot\n",
        "                display.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Video clips are saved in cubic intervals of episodes, so for reference, the first 41 cubes (from https://oeis.org/A000578) are:\n",
        "cubes = [0, 1, 8, 27, 64, 125, 216, 343, 512, 729, 1000, 1331, 1728, 2197, 2744, 3375, 4096, 4913, 5832, 6859, 8000, 9261, 10648, 12167, 13824, 15625, 17576, 19683, 21952, 24389, 27000, 29791, 32768, 35937, 39304, 42875, 46656, 50653, 54872, 59319, 64000]\n",
        "len(cubes)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i3fbD0QRtVWJ"
      },
      "source": [
        "#### Random Agent ####\n",
        "An agent which selects any of the four possible actions randomly at each time step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t86p3I3GtVWJ"
      },
      "outputs": [],
      "source": [
        "# Define random agent class\n",
        "\n",
        "class RandomAgent(LunarLander):\n",
        "    '''\n",
        "    Inherits from lunar lander class\n",
        "    and acts randomly\n",
        "    '''\n",
        "    def __init__(self, name = \"Random Agent\"):\n",
        "        LunarLander.__init__(self, name)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        '''\n",
        "        Randomly select action\n",
        "        '''\n",
        "        # Randomly select action from action space\n",
        "        action = self.env.action_space.sample()\n",
        "\n",
        "        return action"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "24fs2PoStVWJ"
      },
      "source": [
        "#### Simple Reflex Agent ####\n",
        "\n",
        "This is an agent which can fire its side thrusters in reaction to its current horizontal and angular velocity in relation to threshold constants chosen by the programmer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJiSwQLktVWJ"
      },
      "outputs": [],
      "source": [
        "class ReflexAgent(LunarLander):\n",
        "    '''\n",
        "    Inherits from lunar lander class, but \n",
        "    selects actions according to horizontal \n",
        "    and angular position and angular velocity\n",
        "    '''\n",
        "    def __init__(self, name = \"Reflex Agent\"):\n",
        "        LunarLander.__init__(self, name)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        '''\n",
        "        Select action according to horizontal and\n",
        "        angular position and angular velocity\n",
        "        '''\n",
        "        # Set tolerances\n",
        "        horiz_vel_tolerance = 0.4\n",
        "        ang_vel_tolerance = 0.001\n",
        "\n",
        "        # Horizontal velocity\n",
        "        vx = state[3]\n",
        "\n",
        "        # Angular velocity\n",
        "        ang_vel = state[5]\n",
        "\n",
        "        # If leftward horizontal velocity\n",
        "        if ((vx < -horiz_vel_tolerance) and\n",
        "            # and clockwise velocity\n",
        "            (ang_vel > ang_vel_tolerance)):\n",
        "\n",
        "            # Then, fire thruster to move right\n",
        "            action = self.RIGHT\n",
        "        \n",
        "        # If rightward horizontal velocity\n",
        "        elif ((vx > horiz_vel_tolerance) and\n",
        "              # If counter-clockwise velocity\n",
        "              (ang_vel < -ang_vel_tolerance)):\n",
        "            \n",
        "            # Then, fire thruster to move left\n",
        "            action = self.LEFT\n",
        "\n",
        "        else:\n",
        "            # Do nothing\n",
        "            action = self.NOOP\n",
        "        \n",
        "        return action "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F9Sjqo-2tVWJ"
      },
      "source": [
        "#### Deep Q-Network Agent ####\n",
        "\n",
        "A deep Q-network agent has the capacity to remember and learn from its previous experiences using a neural network.\n",
        "\n",
        "##### Replay Memory and Deep Q-Network #####\n",
        "First, we must define a buffer to remember previous experiences and a neural network for the agent to use for learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkRp09t2tVWJ"
      },
      "outputs": [],
      "source": [
        "# Define Replay Memory Buffer and Deep Q-Network\n",
        "# Adapted from: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "# Additional references:\n",
        "# https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html#building-models-with-pytorch\n",
        "# https://pytorch.org/docs/stable/notes/randomness.html\n",
        "\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "# Named tuple data structure to encode a single state transition\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "# Buffer class to hold and randomly sample recent transitions\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        '''\n",
        "        Initialize buffer with empty double-ended queue\n",
        "        of size \"capacity\"\n",
        "        '''\n",
        "        self.memory = deque([], maxlen = capacity)\n",
        "\n",
        "        # Set random seed for reproducibility\n",
        "        random.seed(21)\n",
        "\n",
        "    def push(self, *args):\n",
        "        '''\n",
        "        Add a transition to buffer\n",
        "        '''\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        '''\n",
        "        Randomly sample batch of transitions from buffer\n",
        "        '''\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Returns number of transitions in buffer\n",
        "        '''\n",
        "        return len(self.memory)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Deep Q-Network\n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, features_size, actions_size):\n",
        "        '''\n",
        "        Initialize neural network\n",
        "        '''\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        # First fully-connected layer: \n",
        "        # Input dimensions: 1 x no. of state feature\n",
        "        # Output dimensions: 1 x 128\n",
        "        self.fc1 = nn.Linear(features_size, 128)\n",
        "\n",
        "        # Second fully-connected layer: \n",
        "        # Input: 1 x 128; Output: 1 x 128\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "\n",
        "        # Final fully-connected layer: \n",
        "        # Input: 1 x 128; \n",
        "        # Output: 1 x no. of possible actions\n",
        "        self.fc3 = nn.Linear(128, actions_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        '''\n",
        "        Forward propagation function\n",
        "        Input: State features (e.g., position, velocity, ...)\n",
        "        Output: tensor of action values\n",
        "        '''\n",
        "        # First layer followed by ReLU activation function\n",
        "        z = F.relu(self.fc1(state))\n",
        "        \n",
        "        # Second layer followed by ReLU activation function\n",
        "        z = F.relu(self.fc2(z))\n",
        "        \n",
        "        # Final output layer\n",
        "        return self.fc3(z)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q0SUVYjMtVWK"
      },
      "source": [
        "##### DQN Agent ####\n",
        "Now we can define the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0DyA0d-tVWK"
      },
      "outputs": [],
      "source": [
        "# Define DQN agent\n",
        "# References:\n",
        "# Adapted from: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "# Additional references:\n",
        "# https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html#building-models-with-pytorch\n",
        "# https://pytorch.org/docs/stable/notes/randomness.html\n",
        "# https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n",
        "# https://pytorch.org/docs/stable/generated/torch.max.html\n",
        "# https://stackoverflow.com/questions/19339/transpose-unzip-function-inverse-of-zip/19343\n",
        "# https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
        "# https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html\n",
        "# https://machinelearningmastery.com/using-learning-rate-schedule-in-pytorch-training/\n",
        "\n",
        "\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "class DQNAgent(LunarLander):\n",
        "    '''\n",
        "    Inherits from lunar lander class, but \n",
        "    iteratively learns from experience to approximate \n",
        "    Q-values from state features with a neural network\n",
        "    '''\n",
        "    def __init__(self, \n",
        "                 name = \"DQN Agent\", \n",
        "                 alpha = 0.1,          # initial optimizer learning rate\n",
        "                 epsilon = (1.0,       # epsilon-greedy max\n",
        "                            0.01,      # epsilon-greedy min\n",
        "                            0.001),    # exponential decay rate (smaller = slower)\n",
        "                 gamma = 0.99,         # discount rate\n",
        "                 tau = 0.005,          # target network update rate\n",
        "                 lr_period = 100,      # episodes between learning rate decreases\n",
        "                 lr_decay = 0.1,       # learning rate decay\n",
        "                 batch_size = 128,     # replay buffer batch size\n",
        "                 buffer_size = 1000000 # replay buffer size (no. of transitions)\n",
        "                 ):\n",
        "        '''\n",
        "        Initialize agent hyperparameters and data structures\n",
        "        '''\n",
        "        LunarLander.__init__(self, name)\n",
        "        self.alpha = alpha\n",
        "        self.eps_max, self.eps_min, self.eps_decay = epsilon\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.batch_size = batch_size\n",
        "        self.total_steps = 0\n",
        "\n",
        "        # Set random seeds for reproducibility\n",
        "        torch.manual_seed(21)\n",
        "        random.seed(21)\n",
        "\n",
        "        # Set device to GPU if available\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(\"Device:\",self.device)\n",
        "\n",
        "        # Initialize online and target deep Q-networks \n",
        "        # and move to selected device\n",
        "        self.online_net = DQN(self.features_size, self.actions_size).to(self.device)\n",
        "        self.target_net = DQN(self.features_size, self.actions_size).to(self.device)\n",
        "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
        "\n",
        "        # Initialize optimizer\n",
        "        self.optimizer = optim.AdamW(self.online_net.parameters(), \n",
        "                                lr = self.alpha, # learning rate\n",
        "                                amsgrad = True)  # Reddi et al., 2019 variant\n",
        "        \n",
        "        # Initialize learning rate scheduler\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, lr_period, lr_decay)\n",
        "\n",
        "        # Initialize Huber loss function\n",
        "        self.criterion = nn.SmoothL1Loss()\n",
        "\n",
        "        # Initialize replay memory buffer \n",
        "        # with maximum capacity of buffer_size transitions\n",
        "        self.memory = ReplayMemory(buffer_size)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        '''\n",
        "        Epsilon-greedy action selection with online DQN\n",
        "        '''       \n",
        "        # Calculate exponentially decaying epsilon-greedy threshold\n",
        "        # (as number of steps becomes large, threshold becomes minimum)\n",
        "        eps_threshold = (self.eps_min + (self.eps_max - self.eps_min) \n",
        "                            * math.exp(-self.total_steps * self.eps_decay))\n",
        "        \n",
        "        # Increment total steps\n",
        "        self.total_steps += 1\n",
        "\n",
        "        # Epsilon-greedy algorithm\n",
        "        # If random number [0,1) is greater than epsilon threshold\n",
        "        if (random.random() > eps_threshold):\n",
        "            # Without backward propagation \n",
        "            with torch.no_grad():\n",
        "                # Select action with maximum expected reward \n",
        "                # from online network\n",
        "\n",
        "                # Get index of maximum expected reward\n",
        "                max_value_index = self.online_net(state).max(1)[1]\n",
        "\n",
        "                # Get corresponding action\n",
        "                best_action = max_value_index.view(1,1)\n",
        "\n",
        "                return best_action\n",
        "            \n",
        "        else:\n",
        "            # Randomly select action from action space\n",
        "            # and return as tensor\n",
        "            return torch.tensor([[env.action_space.sample()]], \n",
        "                                device = self.device, dtype = torch.long)\n",
        "    \n",
        "    def optimize_model(self):\n",
        "        '''\n",
        "        Perform single step of optimization on batch of transitions\n",
        "        '''\n",
        "        # If fewer transitions in replay buffer than batch size\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            # Can't run batch yet\n",
        "            # so, just keep collecting transitions\n",
        "            return\n",
        "        \n",
        "        else:\n",
        "            # Randomly select batch of transitions from\n",
        "            # replay buffer\n",
        "            transitions = self.memory.sample(self.batch_size)\n",
        "\n",
        "            # Transpose from batch array of transitions \n",
        "            # to transition of batch-arrays\n",
        "            batch = Transition(*zip(*transitions))\n",
        "\n",
        "            # Mask to select only non-final states \n",
        "            # (i.e., states whose next state is not null)\n",
        "            non_final_mask = torch.tensor(tuple(map(lambda s: \n",
        "                                                    s is not None, \n",
        "                                                    batch.next_state)), \n",
        "                                                    device = self.device, \n",
        "                                                    dtype = torch.bool)\n",
        "\n",
        "            # Concatenate the non-null next states\n",
        "            non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "            \n",
        "            # Concatenate states, actions, and rewards\n",
        "            state_batch = torch.cat(batch.state)\n",
        "            action_batch = torch.cat(batch.action)\n",
        "            reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "            # Calculate online network Q-values for each state and action, Q(s,a) \n",
        "            online_Q_values = self.online_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "            # Initialize tensor of next state expected rewards, V(s') = 0\n",
        "            target_next_state_values = torch.zeros(self.batch_size, device = self.device)\n",
        "                    \n",
        "            # Without back propagation\n",
        "            with torch.no_grad():\n",
        "                # Calculate target network expected reward (i.e., value) \n",
        "                # for each next state, V(s') (post-terminal states will have have value 0)\n",
        "                target_next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n",
        "                           \n",
        "            # Calculate target network expected Q values \n",
        "            target_expected_Q_values = ((target_next_state_values * self.gamma) \n",
        "                                            + reward_batch)\n",
        "\n",
        "            # Calculate loss by comparing online and target network Q values\n",
        "            loss = self.criterion(online_Q_values, \n",
        "                            target_expected_Q_values.unsqueeze(1))\n",
        "\n",
        "            # Reset optimizer gradients\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Back propagation\n",
        "            loss.backward()\n",
        "\n",
        "            # Perform gradient clipping by value to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_value_(self.online_net.parameters(), 100)\n",
        "            \n",
        "            # Adjust learning weights\n",
        "            self.optimizer.step()\n",
        "\n",
        "    def episode(self):\n",
        "        '''\n",
        "        Complete episode in given environment\n",
        "        Call DQN optimizer\n",
        "        Update online and target networks\n",
        "        Return score, steps count, and actions log\n",
        "        '''\n",
        "        # Initialize rewards total\n",
        "        score = 0.0\n",
        "\n",
        "        # Initialize steps count\n",
        "        episode_steps = 0\n",
        "\n",
        "        # Initialize actions list\n",
        "        actions = []\n",
        "\n",
        "        # Start new episode  \n",
        "        state, info = self.env.reset()\n",
        "\n",
        "        #Convert to tensor\n",
        "        state = torch.tensor(state, \n",
        "                            dtype = torch.float32, \n",
        "                            device = self.device).unsqueeze(0)\n",
        "\n",
        "        # Until end of episode\n",
        "        while True:\n",
        "            # Get action based on current state\n",
        "            action = self.select_action(state)\n",
        "\n",
        "            # Obtain next state, reward, terminated status, truncated status, \n",
        "            # and environment info for given action\n",
        "            next_state, reward, terminated, truncated, info = self.env.step(action.item())\n",
        "\n",
        "            # Accumulate reward total\n",
        "            score += reward\n",
        "\n",
        "            # Convert reward to tensor\n",
        "            reward = torch.tensor([reward], device = self.device)\n",
        "\n",
        "            # Increment episode steps\n",
        "            episode_steps += 1\n",
        "\n",
        "            # Increment overall step count\n",
        "            self.total_steps += 1\n",
        "\n",
        "            # Record action\n",
        "            actions.append(action)\n",
        "\n",
        "            # If action taken leads to terminated state\n",
        "            if terminated or truncated:\n",
        "                # Next state is null\n",
        "                next_state = None\n",
        "            else:\n",
        "                # Convert next state to tensor\n",
        "                next_state = torch.tensor(next_state, \n",
        "                                        dtype = torch.float32, \n",
        "                                        device = self.device).unsqueeze(0)\n",
        "\n",
        "            # Store transition in replay buffer\n",
        "            self.memory.push(state, action, next_state, reward)\n",
        "         \n",
        "            # Advance to next state \n",
        "            state = next_state\n",
        "\n",
        "            # Perform one step of optimization on online network\n",
        "            self.optimize_model()            \n",
        "\n",
        "            # Get target and online network state dicts\n",
        "            target_net_state_dict = self.target_net.state_dict()\n",
        "            online_net_state_dict = self.online_net.state_dict()\n",
        "\n",
        "            # For each parameter in the online network state dict\n",
        "            for param in online_net_state_dict:\n",
        "                # Update target network weights at rate tau\n",
        "                # target parameter <-- tau(online parameter) + (1-tau)(target parameter)\n",
        "                target_net_state_dict[param] = (online_net_state_dict[param]*self.tau \n",
        "                                                + target_net_state_dict[param]*(1-self.tau))\n",
        "            \n",
        "            # Load updated target network state dict\n",
        "            self.target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "            # If max. number of steps has been surpassed or end of episode\n",
        "            if (episode_steps > self.step_limit) or terminated or truncated:\n",
        "                break\n",
        "\n",
        "        return score, episode_steps, actions\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "b3xNdtajtVWL"
      },
      "source": [
        "## Results ## \n",
        "\n",
        "In this section, I compare the five different models: an agentless lunar lander, a random agent, a simple reflex agent, and a DQN agent."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m66zGN-UtVWL"
      },
      "source": [
        "### Agentless Lunar Lander ###\n",
        "First, we consider the agentless lunar lander which never fires any of its thrusters and is therefore subject entirely to its environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "KDniF4JqtVWL",
        "outputId": "9435a2d4-8f84-4234-a126-37abc6d44f5c"
      },
      "outputs": [],
      "source": [
        "# Instantiate agentless lunar lander\n",
        "agentless_lander = LunarLander()\n",
        "\n",
        "# Maintain list of agent objects\n",
        "agents = [agentless_lander]\n",
        "\n",
        "episodes = 10\n",
        "\n",
        "# Iterate over selected number of episodes\n",
        "agentless_lander.iterate(episodes, verbose = False, video = True, plot = True)\n",
        "\n",
        "# Print average, max, min\n",
        "average, max, min, avg_length = agentless_lander.score_stats(verbose = True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ojuXxHMxtVWL"
      },
      "source": [
        "[Link to the video of the agentless lander in the 64<sup>th</sup> episode](videos/agentless-lander-episode-64.mp4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nUoU5YJatVWL"
      },
      "source": [
        "In the above graph, we can see that an agentless lunar lander is not sufficient to solve this problem. In the video linked above, we can see the lunar lander simply falling in a slightly curved trajectory toward the ground."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dM7A_JJNtVWL"
      },
      "source": [
        "### Random Agent ###\n",
        "\n",
        "Next, we consider the random agent which selects an action randomly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "pR2-uokwtVWL",
        "outputId": "bb9668be-e851-4946-a00e-7ee5afb8c282"
      },
      "outputs": [],
      "source": [
        "# Instantiate lunar lander random agent\n",
        "rand_agent = RandomAgent()\n",
        "\n",
        "# Add to list of agents\n",
        "agents.append(rand_agent)\n",
        "\n",
        "# Iterate over episodes\n",
        "rand_agent.iterate(episodes, verbose = False, video = True)\n",
        "\n",
        "# Print average, max, min\n",
        "average, max, min, avg_length = rand_agent.score_stats(verbose = True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "b1S8c-nTtVWM"
      },
      "source": [
        "[Link to the video of the random agent in the 64<sup>th</sup> episode](videos/random-agent-episode-64.mp4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nWzOXuJtVWM"
      },
      "outputs": [],
      "source": [
        "def plot_comparison(agents):\n",
        "    '''\n",
        "    Plot scores of multiple agent objects\n",
        "    '''\n",
        "    # Initialize plot\n",
        "    plt.figure(1)\n",
        "    plt.ylim([-600,300])\n",
        "\n",
        "    # Plot solution threshold\n",
        "    plt.axhline(y = 200, color = \"r\", linestyle = \"dashed\", label = \"Solution Threshold\")  \n",
        "\n",
        "    # Initialize list of agent names\n",
        "    names = []\n",
        "\n",
        "    # Iterate over agents list\n",
        "    for agent in agents:\n",
        "        # Plot agent's scores\n",
        "        plt.plot(agent.scores, label = agent.name)\n",
        "        names.append(agent.name)\n",
        "\n",
        "    plt.title(\", \".join(names))\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.legend()\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "svouIuUPtVWM",
        "outputId": "eaa692b7-876b-42ec-abbb-230d5de7f4c5"
      },
      "outputs": [],
      "source": [
        "# Plot agentless lander and random agent scores over all episodes\n",
        "plot_comparison(agents)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "07Vkx0vTtVWM"
      },
      "source": [
        "As the above graph shows, doing nothing is better than acting randomly in this environment. The agentless lunar lander is relatively more consistent and successful compared to the random agent and also has a slightly higher average score. Obviously, neither learn anything nor come remotely close to consistently reaching the solution threshold, as expected."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "682LCasFtVWM"
      },
      "source": [
        "### Simple Reflex Agent ###\n",
        "\n",
        "Next, we examine the agent which can reflexively fire its side engines in reaction to its current horizontal and angular velocity mediated by pre-selected threshold constants. Choosing ideal threshold constants requires tedious trial and error on the part of the programmer, rather than the agent. Thus, only the programmer is learning in this scenario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "1tqQ5PhctVWM",
        "outputId": "184de646-28a0-46b9-fb8d-1a1946b2df18"
      },
      "outputs": [],
      "source": [
        "# Instantiate lunar lander reflex agent\n",
        "rflx_agent = ReflexAgent()\n",
        "\n",
        "# Add to list of agents\n",
        "agents.append(rflx_agent)\n",
        "\n",
        "# Iterate over episodes\n",
        "rflx_agent.iterate(episodes, verbose = False, video = True)\n",
        "\n",
        "# Print average max, min\n",
        "average, max, min, avg_length = rflx_agent.score_stats(verbose = True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FYHBv_J9tVWN"
      },
      "source": [
        "[Link to the video of the reflex agent in the 64<sup>th</sup> episode](videos/reflex-agent-episode-64.mp4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "-VljD0TqtVWN",
        "outputId": "1918e035-625d-48af-e704-741d9cc4e17f"
      },
      "outputs": [],
      "source": [
        "# Plot comparison of agents\n",
        "plot_comparison(agents)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qB4J_Jb5tVWN"
      },
      "source": [
        "In the above graph, we can see that the reflex agent is only slightly more consistent and successful than the agentless lunar lander but still not even close to solving the problem. Additionally, it is clearly not learning, as expected. Landing a lunar rover appears to be more difficult than firing the left and right thrusters based on horizontal and angular velocity. It might be possible to achieve a higher score by comparing various algorithms and parameters through trial and error, but this would be a tedious process, so we would like to find a more expedient way to find the best policy to follow."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "idym3iPetVWN"
      },
      "source": [
        "### Deep Q-Network Agent ###\n",
        "\n",
        "Next, we examine the performance of a remembering, learning deep Q-network agent. We'll start with the following hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hW65uXBztVWN"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "name = \"DQN Agent 1\"\n",
        "alpha = 0.1             # initial optimizer learning rate\n",
        "epsilon = (0.99,        # epsilon-greedy max (starting value)\n",
        "        0.01,           # epsilon-greedy min (ending value)\n",
        "        0.001)          # exponential decay rate (smaller = slower)\n",
        "gamma = 0.99            # discount rate\n",
        "tau = 0.005             # target network update rate\n",
        "lr_period = 100         # episodes between learning rate decreases\n",
        "lr_decay = 0.1          # learning rate decay\n",
        "batch_size = 128        # replay buffer batch size\n",
        "buffer_size = 1000000   # replay buffer size (no. of transitions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's confirm the neural network architecture and estimated computation requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OtAzYvltVWN",
        "outputId": "bc157c8c-224f-459a-b62e-697f5fa21540"
      },
      "outputs": [],
      "source": [
        "# Instantiate lunar lander DQN agent\n",
        "dqn_agent1 = DQNAgent(name, alpha, epsilon, gamma, tau, lr_period, lr_decay, batch_size, buffer_size)\n",
        "\n",
        "# View model summary\n",
        "# https://github.com/TylerYep/torchinfo\n",
        "# https://discuss.pytorch.org/t/how-to-check-if-model-is-on-cuda/180/7\n",
        "\n",
        "print(\"Model parameters on CUDA?\",next(dqn_agent1.online_net.parameters()).is_cuda)\n",
        "print(dqn_agent1.online_net)\n",
        "\n",
        "from torchinfo import summary\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "summary(dqn_agent1.online_net, \n",
        "        input_size = (dqn_agent1.batch_size, dqn_agent1.features_size))\n",
        "\n",
        "warnings.filterwarnings(\"default\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 811
        },
        "id": "4R7Bm5MmtVWO",
        "outputId": "f58e16be-e4ee-4998-91d1-7c2b5d52574d"
      },
      "outputs": [],
      "source": [
        "# Train DQN agent\n",
        "\n",
        "# If GPU device is available\n",
        "if torch.cuda.is_available():\n",
        "    episodes = 1000\n",
        "    print(\"Device: cuda\")\n",
        "else:\n",
        "    episodes = 600\n",
        "    print(\"Device: cpu\")\n",
        "\n",
        "# Train for specified number of episodes\n",
        "dqn_agent1.iterate(episodes, verbose = False, video = True)\n",
        "\n",
        "# Print average, max, min\n",
        "average, max, min, avg_length = dqn_agent1.score_stats(verbose = True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5orcJHXytVWO"
      },
      "source": [
        "[Link to the video of the first DQN agent in the 512<sup>th</sup> episode](videos/dqn-agent-1-episode-512.mp4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This agent seems to have already begun settling on a policy that resulted in scores worse than the reflex agent. If we look at the video linked above, we see that the agent seems to be trying to hover at the top center of the screen to accumulate rewards by being close to center on the horizontal axis (albeit far from the landing pad at the origin and not taking advantage of the 100-point landing reward). It has learned a strategy, but not the strategy we want it to learn.\n",
        "\n",
        "Let's give the agent some more incentive to stop hovering in the air and get to the business of landing by lowering the discount rate. This will lead to diminishing returns for just hovering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjRuEovVtVWO"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "name = \"DQN Agent 2\"\n",
        "alpha = 0.1             # initial optimizer learning rate\n",
        "epsilon = (0.99,        # epsilon-greedy max (starting value)\n",
        "        0.01,           # epsilon-greedy min (ending value)\n",
        "        0.001)          # exponential decay rate (smaller = slower)\n",
        "gamma = 0.8             # discount rate\n",
        "tau = 0.005             # target network update rate\n",
        "lr_period = 100         # episodes between learning rate decreases\n",
        "lr_decay = 0.1          # learning rate decay\n",
        "batch_size = 128        # replay buffer batch size\n",
        "buffer_size = 1000000   # replay buffer size (no. of transitions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlOHE4HdtVWO"
      },
      "outputs": [],
      "source": [
        "# Instantiate second lunar lander DQN agent\n",
        "dqn_agent2 = DQNAgent(name, alpha, epsilon, gamma, tau, lr_period, lr_decay, batch_size, buffer_size)\n",
        "\n",
        "# Train for specified number of episodes\n",
        "dqn_agent2.iterate(episodes, verbose = False, video = True)\n",
        "\n",
        "# Print average, max, min\n",
        "average, max, min = dqn_agent2.score_stats(verbose = True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Link to the video of the second DQN agent in the 512<sup>th</sup> episode](videos/dqn-agent-2-episode-512.mp4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AEMO6SRdtVWP"
      },
      "source": [
        "This seems more promising. The average is lower, but there is more exploration. We can see more fluctation in the running average, too. This exploration of choices beyond hovering in the top center has already led it toward discovering better choices. Also, the video linked above shows the agent landing instead of hovering. It still needs some refinement, but it's already much more controlled than the non-learning agents. If we run the training longer, this agent might actually find the solution, but let's try changing a few other hyperparameters before we commit the time to train longer. \n",
        "\n",
        "Let's try increasing the target network update rate $\\tau$ next. This should speed up training, but it might also create instability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfVIMEPDtVWP"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "name = \"DQN Agent 3\"\n",
        "alpha = 0.1             # initial optimizer learning rate\n",
        "epsilon = (0.99,        # epsilon-greedy max (starting value)\n",
        "        0.01,           # epsilon-greedy min (ending value)\n",
        "        0.001)          # exponential decay rate (smaller = slower)\n",
        "gamma = 0.9             # discount rate\n",
        "tau = 0.01              # target network update rate\n",
        "lr_period = 100         # episodes between learning rate decreases\n",
        "lr_decay = 0.1          # learning rate decay\n",
        "batch_size = 128        # replay buffer batch size\n",
        "buffer_size = 1000000   # replay buffer size (no. of transitions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITi_JD00tVWP"
      },
      "outputs": [],
      "source": [
        "# Instantiate lunar lander DQN agent\n",
        "dqn_agent3 = DQNAgent(name, alpha, epsilon, gamma, tau, lr_period, lr_decay, batch_size, buffer_size)\n",
        "\n",
        "# Train for specified number of episodes\n",
        "dqn_agent3.iterate(episodes, verbose = False, video = True)\n",
        "\n",
        "# Print average, max, min\n",
        "average, max, min, avg_length = dqn_agent3.score_stats(verbose = True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Link to the video of the third DQN agent in the 512<sup>th</sup> episode](videos/dqn-agent-3-episode-512.mp4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UnuBEqPZtVWP"
      },
      "source": [
        "The average score was lower this time. The fluctations seen in DQN Agent 2's running average have been smoothed out. This demonstrates the stabilizing effect of dampening or slowing the updates to the target network. In the video, it seems that the agent has not progressed as far in its learning compared to the previous agent. Let's go back to the previous target network update rate for now.\n",
        "\n",
        "Let's try decreasing the greedy-epsilon decay rate next. This should encourage even more exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IT-fpSStVWP"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "name = \"DQN Agent 4\"\n",
        "alpha = 0.1             # initial optimizer learning rate\n",
        "epsilon = (0.99,        # epsilon-greedy max (starting value)\n",
        "        0.01,           # epsilon-greedy min (ending value)\n",
        "        0.0001)         # exponential decay rate (smaller = slower)\n",
        "gamma = 0.9             # discount rate\n",
        "tau = 0.005             # target network update rate\n",
        "lr_period = 100         # episodes between learning rate decreases\n",
        "lr_decay = 0.1          # learning rate decay\n",
        "batch_size = 128        # replay buffer batch size\n",
        "buffer_size = 1000000   # replay buffer size (no. of transitions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOvuDvlltVWP"
      },
      "outputs": [],
      "source": [
        "# Instantiate lunar lander DQN agent\n",
        "dqn_agent4 = DQNAgent(name, alpha, epsilon, gamma, tau, lr_period, lr_decay, batch_size, buffer_size)\n",
        "\n",
        "# Train for specified number of episodes\n",
        "dqn_agent4.iterate(episodes, verbose = False, video = True)\n",
        "\n",
        "# Print average, max, min\n",
        "average, max, min, avg_length = dqn_agent4.score_stats(verbose = True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Link to the video of the fourth DQN agent in the 512<sup>th</sup> episode](videos/dqn-agent-4-episode-512.mp4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The average is slightly higher than the second DQN agent. There also seems to be more variation. In the video, the agent landed crashed outside the landing pad. This seems to just be prolonging the learning process.\n",
        "\n",
        "Let's return to the second agent and see what happens when we train it longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "name = \"DQN Agent 2\"\n",
        "alpha = 0.1             # initial optimizer learning rate\n",
        "epsilon = (0.99,        # epsilon-greedy max (starting value)\n",
        "        0.01,           # epsilon-greedy min (ending value)\n",
        "        0.001)          # exponential decay rate (smaller = slower)\n",
        "gamma = 0.9             # discount rate\n",
        "tau = 0.005             # target network update rate\n",
        "lr_period = 100         # episodes between learning rate decreases\n",
        "lr_decay = 0.1          # learning rate decay\n",
        "batch_size = 128        # replay buffer batch size\n",
        "buffer_size = 1000000   # replay buffer size (no. of transitions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reset the second lunar lander DQN agent\n",
        "dqn_agent2 = DQNAgent(name, alpha, epsilon, gamma, tau, lr_period, lr_decay, batch_size, buffer_size)\n",
        "  \n",
        "# If GPU device is available\n",
        "if torch.cuda.is_available():\n",
        "    episodes = 1000\n",
        "    print(\"Device: cuda\")\n",
        "else:\n",
        "    episodes = 600\n",
        "    print(\"Device: cpu\")\n",
        "\n",
        "# Train for specified number of episodes\n",
        "dqn_agent2.iterate(episodes, verbose = False, video = True)\n",
        "\n",
        "# Print average, max, min\n",
        "average, max, min, avg_length = dqn_agent2.score_stats(verbose = True)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Link to the video of the second DQN agent in the 512<sup>th</sup> episode](videos/dqn-agent-2-episode-512.mp4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Bab405_OtVWQ"
      },
      "source": [
        "Summary"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GrzGnJRctVWQ"
      },
      "source": [
        "## Discussion ## \n",
        "\n",
        "Does it include discussion (what went well or not and why), and suggestions for improvements or future work?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "b5V6pQeQtVWQ"
      },
      "source": [
        "### Suggestions for Future Research ###\n",
        "\n",
        "There are a number of directions for future research:\n",
        "\n",
        "I have only explored a few optimizations in this paper. There are still many ways to improve upon the model presented here, ranging from more hyperparameter tuning to the exploration of different optimization algorithms and neural network architectures. More rigorous model testing is also needed. In addition, the Gymnasium lunar lander environment is deterministic. In such cases, Machado and his colleagues (2018) recommend introducing stochasticity with frame skipping and sticky actions that repeat with a given probability in order to explore the robustness of models in more realistic environments. Another direction for exploration is applying convolutional neural networks and comparing the results with this state feature based model.\n",
        "\n",
        "More complex Gymnasium environments, such as the \"Arcade Learning Environment\" (Bellemare, 2023; Bellemare et al., 2013; Machado et al., 2018), which simulates classic Atari 2600 games (AtariAge, 2023), can also provide opportunities to explore more challenging problems. \n",
        "\n",
        "### Conclusion ###\n",
        "This project compared four different artificial agents in a lunar lander simulation. As expected, the deep reinforcement learning agent performed better than the three non-learning agents. Of the three non-learning agents, the simple reflex agent was only slightly better than an agentless lunar lander, and acting randomly was the worst strategy. A deep Q-network with two fully-connected neural network layers was used for the learning agent. Hyperparameter tuning resulted in\n",
        "\n",
        "Finally, it is only natural to ask: What is the value of crashing a virtual lunar lander model hundreds of times in order to eventually land successfully? There are two main reasons. First, it is better to train a far less-expensive virtual model and then transfer that learning to a real-world environment. Thus, the long-term aim is to develop the ability to transfer this virtual knowledge to a real-world environment. Second, through this project, I have gained skills and experience with deep reinforcement learning that I can build on to explore more sophisticated artificial intelligence algorithms in the future."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SHXvsejctVWQ"
      },
      "source": [
        "## References ##\n",
        "\n",
        "_AtariAge._ (2023). [Website]. [https://atariage.com](https://atariage.com)\n",
        "\n",
        "Bellemare, M. G. (2023). Arcade Learning Environment. [https://github.com/mgbellemare/Arcade-Learning-Environment](https://github.com/mgbellemare/Arcade-Learning-Environment)\n",
        "\n",
        "Bellemare, M. G., Naddaf, Y., Veness, J., & Bowling, M. (2013). The Arcade Learning Environment: An evaluation platform for general agents. _Journal of Artificial Intelligence Research, 47,_ 253-279.\n",
        "[https://doi.org/10.1613/jair.3912](https://doi.org/10.1613/jair.3912)\n",
        "\n",
        "Brownlee, J. (2017, July 3). Gentle introduction to the Adam optimization algorithm for deep learning. _Machine Learning Mastery._ [https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
        "\n",
        "Goodfellow, I., Bengio, Y., & Courville, A. (2016). _Deep learning._ MIT Press. [http://www.deeplearningbook.org](http://www.deeplearningbook.org)\n",
        "\n",
        "_Gymnasium._ (2022). Farama Foundation. [https://gymnasium.farama.org/](https://gymnasium.farama.org/)\n",
        "\n",
        "Heintz, B. (2021, April 17). _Building models with Pytorch._ Pytorch. [https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html#building-models-with-pytorch](https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html#building-models-with-pytorch)\n",
        "\n",
        "Huber, P. J. (1964). Robust estimation of a location parameter. _Annals of Statistics, 53_(1), 73–101. [https//doi.org/10.1214/aoms/1177703732](https//doi.org/10.1214/aoms/1177703732)\n",
        "\n",
        "Karpathy, A. (2016). _Convolutional neural networks for visual recognition._ Stanford University. [https://cs231n.github.io/convolutional-networks/](https://cs231n.github.io/convolutional-networks/)\n",
        "\n",
        "Loshchilov, I., & Hutter, F. (2019). Decoupled weight decay regularization. _International Conference on Learning Representations._[https://doi.org/10.48550/arXiv.1711.05101](https://doi.org/10.48550/arXiv.1711.05101)\n",
        "\n",
        "Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., & Bowling, M. (2018). Revisiting the Arcade Learning Environment: Evaluation protocols and open problems for general agents. _Journal of Artificial Intelligence Research, 61,_ 523–562. [https://doi.org/10.1613/jair.5699](https://doi.org/10.1613/jair.5699)\n",
        "\n",
        "Matplotlib. (2023). _API reference._ [https://matplotlib.org/stable/api/index](https://matplotlib.org/stable/api/index)\n",
        "\n",
        "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., & Hassabis, D. (2015). Human-level control through deep reinforcement learning. _Nature, 518_, 529–533. [https://doi.org/10.1038/nature14236](https://doi.org/10.1038/nature14236)\n",
        "\n",
        "Mnih, V., Puigdomènech Badia, A., Mirza, M., Graves, A., Harley, T., Lillicrap, T. P., Silver, D., & Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. _Proceedings of the 33rd International Conference on Machine Learning, 48,_ 1928–1937. [https://proceedings.mlr.press/v48/mniha16.html](https://proceedings.mlr.press/v48/mniha16.html)\n",
        "\n",
        "Morales, M. (2020). _Grokking deep reinforcement learning._ Simon and Schuster.\n",
        "\n",
        "Paszke, A., & Towers, M. (2017, April 6). _Reinforcement learning (DQN) tutorial._ Pytorch. [https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
        "\n",
        "Pytorch. (2023). _Pytorch documentation._ [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)\n",
        "\n",
        "Reddi, S. J., Kale, S., & Kumar, S. (2019). On the convergence of adam and beyond. _International Conference on Learning Representations._[https://doi.org/10.48550/arXiv.1904.09237](https://doi.org/10.48550/arXiv.1904.09237)\n",
        "\n",
        "Russell, S., & Norvig, R. (2022). _Artificial intelligence: A modern approach,_ (4th ed.). Pearson. \n",
        "\n",
        "Sutton, R. S., & Barto, A. G. (2018). _Reinforcement learning: An introduction._ MIT Press.\n",
        "\n",
        "Tam, A. (2023, February 22). Using learning rate schedule in Pytorch training. _Machine Learning Mastery._ [https://machinelearningmastery.com/using-learning-rate-schedule-in-pytorch-training/](https://machinelearningmastery.com/using-learning-rate-schedule-in-pytorch-training/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
